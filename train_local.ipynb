{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413d2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import os\n",
    "from transformer_flow import Model\n",
    "import utils\n",
    "import pathlib\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" # set GPU\n",
    "\n",
    "utils.set_random_seed(100)\n",
    "notebook_output_path = pathlib.Path('runs/notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f99e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(x, lo, hi): # WuYi\n",
    "    \"\"\"Rescale a tensor to [lo,hi].\"\"\"\n",
    "    assert (lo < hi), f\"[rescale] lo={lo} must be smaller than hi={hi}\"\n",
    "    old_width = torch.max(x) - torch.min(x)\n",
    "    old_center = torch.min(x) + (old_width / 2.)\n",
    "    new_width = float(hi - lo)\n",
    "    new_center = lo + (new_width / 2.)\n",
    "    # shift everything back to zero:\n",
    "    x = x - old_center\n",
    "    # rescale to correct width:\n",
    "    x = x * (new_width / old_width)\n",
    "    # shift everything to the new center:\n",
    "    x = x + new_center\n",
    "    # return:\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5c805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n",
      "Number of parameters: 3.23M\n",
      "epoch 0 lr 0.002000 loss -0.0959\n",
      "layer norms 3.1774 2.8396 1.6584 0.8780\n",
      "logdet: 0.8557, prior p: 0.4390\n",
      "\n",
      "\n",
      "epoch 1 lr 0.001999 loss -0.4592\n",
      "layer norms 3.1157 2.7451 1.6982 0.9084\n",
      "logdet: 0.9933, prior p: 0.4542\n",
      "\n",
      "\n",
      "epoch 2 lr 0.001998 loss -0.5306\n",
      "layer norms 3.3822 3.0416 2.0015 1.1273\n",
      "logdet: 1.0928, prior p: 0.5637\n",
      "\n",
      "\n",
      "epoch 3 lr 0.001995 loss -0.5668\n",
      "layer norms 3.2899 3.0536 1.9957 1.0206\n",
      "logdet: 1.1065, prior p: 0.5103\n",
      "\n",
      "\n",
      "epoch 4 lr 0.001992 loss -0.5813\n",
      "layer norms 3.6279 3.5082 2.2640 0.9384\n",
      "logdet: 1.0053, prior p: 0.4692\n",
      "\n",
      "\n",
      "epoch 5 lr 0.001987 loss -0.5940\n",
      "layer norms 3.1869 3.0346 2.0453 0.9632\n",
      "logdet: 1.1404, prior p: 0.4816\n",
      "\n",
      "\n",
      "epoch 6 lr 0.001982 loss -0.5990\n",
      "layer norms 3.6849 3.7775 2.6632 0.9648\n",
      "logdet: 1.0428, prior p: 0.4824\n",
      "\n",
      "\n",
      "epoch 7 lr 0.001975 loss -0.6034\n",
      "layer norms 3.6045 3.8020 2.5681 0.9280\n",
      "logdet: 1.0465, prior p: 0.4640\n",
      "\n",
      "\n",
      "epoch 8 lr 0.001968 loss -0.6057\n",
      "layer norms 3.6082 3.8073 2.8070 0.9810\n",
      "logdet: 1.1007, prior p: 0.4905\n",
      "\n",
      "\n",
      "epoch 9 lr 0.001960 loss -0.6124\n",
      "layer norms 3.4437 3.6551 2.6995 0.9680\n",
      "logdet: 1.1169, prior p: 0.4840\n",
      "sampling complete. Sample mean: 0.2647, std: 0.2235, max: 0.9906, min: 0.0045\n",
      "latent mean: 0.4989, std: 0.2051\n",
      "\n",
      "\n",
      "epoch 10 lr 0.001950 loss -0.6117\n",
      "layer norms 3.5848 3.8828 2.9750 1.0568\n",
      "logdet: 1.1425, prior p: 0.5284\n",
      "\n",
      "\n",
      "epoch 11 lr 0.001940 loss -0.6196\n",
      "layer norms 4.0236 4.5698 3.4268 1.0512\n",
      "logdet: 1.0794, prior p: 0.5256\n",
      "\n",
      "\n",
      "epoch 12 lr 0.001928 loss -0.6189\n",
      "layer norms 3.5317 3.9441 2.9387 0.9270\n",
      "logdet: 1.0927, prior p: 0.4635\n",
      "\n",
      "\n",
      "epoch 13 lr 0.001916 loss -0.6240\n",
      "layer norms 3.3946 3.8653 2.9762 0.9562\n",
      "logdet: 1.1407, prior p: 0.4781\n",
      "\n",
      "\n",
      "epoch 14 lr 0.001903 loss -0.6260\n",
      "layer norms 3.7825 4.4966 3.6051 1.0540\n",
      "logdet: 1.1257, prior p: 0.5270\n",
      "\n",
      "\n",
      "epoch 15 lr 0.001889 loss -0.6288\n",
      "layer norms 3.5784 4.2551 3.3756 1.0059\n",
      "logdet: 1.1465, prior p: 0.5029\n",
      "\n",
      "\n",
      "epoch 16 lr 0.001874 loss -0.6210\n",
      "layer norms 3.4832 4.0135 3.2654 1.0060\n",
      "logdet: 1.1657, prior p: 0.5030\n",
      "\n",
      "\n",
      "epoch 17 lr 0.001858 loss -0.6240\n",
      "layer norms 4.0205 5.0597 4.0138 1.0191\n",
      "logdet: 1.0803, prior p: 0.5096\n",
      "\n",
      "\n",
      "epoch 18 lr 0.001841 loss -0.6255\n",
      "layer norms 3.8764 4.7514 3.7346 0.9986\n",
      "logdet: 1.1099, prior p: 0.4993\n",
      "\n",
      "\n",
      "epoch 19 lr 0.001824 loss -0.6261\n",
      "layer norms 3.7466 4.5052 3.7749 1.0448\n",
      "logdet: 1.1418, prior p: 0.5224\n",
      "sampling complete. Sample mean: 0.2620, std: 0.2269, max: 0.9846, min: 0.0064\n",
      "latent mean: 0.5003, std: 0.2102\n",
      "\n",
      "\n",
      "epoch 20 lr 0.001805 loss -0.6365\n",
      "layer norms 3.9520 5.0686 4.1432 0.9935\n",
      "logdet: 1.1016, prior p: 0.4967\n",
      "\n",
      "\n",
      "epoch 21 lr 0.001786 loss -0.6352\n",
      "layer norms 4.2733 5.6163 4.4278 1.0351\n",
      "logdet: 1.0914, prior p: 0.5176\n",
      "\n",
      "\n",
      "epoch 22 lr 0.001766 loss -0.6297\n",
      "layer norms 3.6284 4.4178 3.6519 1.0021\n",
      "logdet: 1.1576, prior p: 0.5011\n",
      "\n",
      "\n",
      "epoch 23 lr 0.001745 loss -0.6353\n",
      "layer norms 4.3502 5.6573 4.5578 1.0523\n",
      "logdet: 1.0731, prior p: 0.5261\n",
      "\n",
      "\n",
      "epoch 24 lr 0.001724 loss -0.6319\n",
      "layer norms 3.8637 4.9924 4.0757 0.9945\n",
      "logdet: 1.1318, prior p: 0.4972\n",
      "\n",
      "\n",
      "epoch 25 lr 0.001702 loss -0.6349\n",
      "layer norms 3.8351 4.8999 3.9552 0.9955\n",
      "logdet: 1.1432, prior p: 0.4977\n",
      "\n",
      "\n",
      "epoch 26 lr 0.001679 loss -0.6393\n",
      "layer norms 3.4373 4.3551 3.6996 1.0390\n",
      "logdet: 1.2204, prior p: 0.5195\n",
      "\n",
      "\n",
      "epoch 27 lr 0.001655 loss -0.6346\n",
      "layer norms 3.6512 4.5660 3.8583 1.0145\n",
      "logdet: 1.1677, prior p: 0.5072\n",
      "\n",
      "\n",
      "epoch 28 lr 0.001631 loss -0.6376\n",
      "layer norms 3.9496 5.1507 4.3100 1.0071\n",
      "logdet: 1.1328, prior p: 0.5035\n",
      "\n",
      "\n",
      "epoch 29 lr 0.001606 loss -0.6364\n",
      "layer norms 3.9284 5.1198 4.1381 0.9606\n",
      "logdet: 1.1139, prior p: 0.4803\n",
      "sampling complete. Sample mean: 0.2491, std: 0.2234, max: 0.9886, min: 0.0075\n",
      "latent mean: 0.4955, std: 0.2042\n",
      "\n",
      "\n",
      "epoch 30 lr 0.001580 loss -0.6392\n",
      "layer norms 3.6711 4.6389 3.9387 0.9990\n",
      "logdet: 1.1698, prior p: 0.4995\n",
      "\n",
      "\n",
      "epoch 31 lr 0.001554 loss -0.6403\n",
      "layer norms 4.0714 5.3451 4.3704 0.9859\n",
      "logdet: 1.1081, prior p: 0.4930\n",
      "\n",
      "\n",
      "epoch 32 lr 0.001527 loss -0.6424\n",
      "layer norms 4.0276 5.2930 4.3543 1.0131\n",
      "logdet: 1.1342, prior p: 0.5066\n",
      "\n",
      "\n",
      "epoch 33 lr 0.001500 loss -0.6355\n",
      "layer norms 3.6230 4.6284 3.9063 0.9735\n",
      "logdet: 1.1731, prior p: 0.4868\n",
      "\n",
      "\n",
      "epoch 34 lr 0.001473 loss -0.6399\n",
      "layer norms 4.1483 5.5454 4.5961 1.0243\n",
      "logdet: 1.1237, prior p: 0.5121\n",
      "\n",
      "\n",
      "epoch 35 lr 0.001444 loss -0.6384\n",
      "layer norms 4.5042 6.3073 5.2347 1.0252\n",
      "logdet: 1.0769, prior p: 0.5126\n",
      "\n",
      "\n",
      "epoch 36 lr 0.001416 loss -0.6446\n",
      "layer norms 3.8132 5.0096 4.2734 0.9919\n",
      "logdet: 1.1592, prior p: 0.4960\n",
      "\n",
      "\n",
      "epoch 37 lr 0.001387 loss -0.6419\n",
      "layer norms 4.4571 6.1222 5.0782 1.0641\n",
      "logdet: 1.0929, prior p: 0.5320\n",
      "\n",
      "\n",
      "epoch 38 lr 0.001357 loss -0.6456\n",
      "layer norms 3.3820 4.2895 3.7628 0.9445\n",
      "logdet: 1.2018, prior p: 0.4722\n",
      "\n",
      "\n",
      "epoch 39 lr 0.001327 loss -0.6453\n",
      "layer norms 4.0813 5.6192 4.6964 0.9742\n",
      "logdet: 1.1194, prior p: 0.4871\n",
      "sampling complete. Sample mean: 0.2484, std: 0.2159, max: 0.9850, min: 0.0057\n",
      "latent mean: 0.4996, std: 0.2054\n",
      "\n",
      "\n",
      "epoch 40 lr 0.001297 loss -0.6486\n",
      "layer norms 3.6919 4.8752 4.2189 0.9655\n",
      "logdet: 1.1843, prior p: 0.4828\n",
      "\n",
      "\n",
      "epoch 41 lr 0.001267 loss -0.6423\n",
      "layer norms 4.0738 5.5833 4.6918 0.9936\n",
      "logdet: 1.1420, prior p: 0.4968\n",
      "\n",
      "\n",
      "epoch 42 lr 0.001236 loss -0.6476\n",
      "layer norms 3.6849 4.9150 4.2663 0.9471\n",
      "logdet: 1.1777, prior p: 0.4735\n",
      "\n",
      "\n",
      "epoch 43 lr 0.001205 loss -0.6451\n",
      "layer norms 3.4673 4.5835 4.1487 0.9899\n",
      "logdet: 1.2421, prior p: 0.4950\n",
      "\n",
      "\n",
      "epoch 44 lr 0.001174 loss -0.6456\n",
      "layer norms 3.9247 5.3854 4.7641 1.0140\n",
      "logdet: 1.1715, prior p: 0.5070\n",
      "\n",
      "\n",
      "epoch 45 lr 0.001143 loss -0.6432\n",
      "layer norms 4.6251 6.5934 5.5465 1.0434\n",
      "logdet: 1.0873, prior p: 0.5217\n",
      "\n",
      "\n",
      "epoch 46 lr 0.001111 loss -0.6456\n",
      "layer norms 4.0267 5.5581 4.8351 1.0314\n",
      "logdet: 1.1669, prior p: 0.5157\n",
      "\n",
      "\n",
      "epoch 47 lr 0.001080 loss -0.6471\n",
      "layer norms 4.3648 6.1079 5.0310 0.9724\n",
      "logdet: 1.0944, prior p: 0.4862\n",
      "\n",
      "\n",
      "epoch 48 lr 0.001048 loss -0.6461\n",
      "layer norms 4.3045 6.0919 5.1322 0.9892\n",
      "logdet: 1.1186, prior p: 0.4946\n",
      "\n",
      "\n",
      "epoch 49 lr 0.001016 loss -0.6449\n",
      "layer norms 4.1249 5.6866 4.8130 0.9790\n",
      "logdet: 1.1156, prior p: 0.4895\n",
      "sampling complete. Sample mean: 0.2547, std: 0.2270, max: 0.9929, min: 0.0075\n",
      "latent mean: 0.4991, std: 0.2062\n",
      "\n",
      "\n",
      "epoch 50 lr 0.000985 loss -0.6536\n",
      "layer norms 3.9730 5.3701 4.5751 0.9829\n",
      "logdet: 1.1460, prior p: 0.4914\n",
      "\n",
      "\n",
      "epoch 51 lr 0.000953 loss -0.6473\n",
      "layer norms 4.8714 6.9501 5.8305 1.0465\n",
      "logdet: 1.0509, prior p: 0.5232\n",
      "\n",
      "\n",
      "epoch 52 lr 0.000921 loss -0.6504\n",
      "layer norms 4.5245 6.4455 5.5321 1.0465\n",
      "logdet: 1.1069, prior p: 0.5233\n",
      "\n",
      "\n",
      "epoch 53 lr 0.000890 loss -0.6540\n",
      "layer norms 4.0451 5.6443 4.8864 0.9965\n",
      "logdet: 1.1539, prior p: 0.4982\n",
      "\n",
      "\n",
      "epoch 54 lr 0.000858 loss -0.6524\n",
      "layer norms 4.4782 6.3357 5.3011 1.0193\n",
      "logdet: 1.1050, prior p: 0.5096\n",
      "\n",
      "\n",
      "epoch 55 lr 0.000827 loss -0.6538\n",
      "layer norms 4.0297 5.5908 4.8548 0.9882\n",
      "logdet: 1.1491, prior p: 0.4941\n",
      "\n",
      "\n",
      "epoch 56 lr 0.000796 loss -0.6472\n",
      "layer norms 4.1697 5.8318 5.0692 1.0235\n",
      "logdet: 1.1388, prior p: 0.5118\n",
      "\n",
      "\n",
      "epoch 57 lr 0.000765 loss -0.6524\n",
      "layer norms 4.2621 6.0230 5.2539 1.0360\n",
      "logdet: 1.1304, prior p: 0.5180\n",
      "\n",
      "\n",
      "epoch 58 lr 0.000734 loss -0.6459\n",
      "layer norms 4.0392 5.5590 4.8467 1.0016\n",
      "logdet: 1.1436, prior p: 0.5008\n",
      "\n",
      "\n",
      "epoch 59 lr 0.000704 loss -0.6609\n",
      "layer norms 3.3974 4.5618 4.2640 0.9951\n",
      "logdet: 1.2505, prior p: 0.4975\n",
      "sampling complete. Sample mean: 0.2508, std: 0.2185, max: 0.9805, min: 0.0115\n",
      "latent mean: 0.5077, std: 0.2073\n",
      "\n",
      "\n",
      "epoch 60 lr 0.000674 loss -0.6552\n",
      "layer norms 3.8368 5.3486 4.6203 0.9690\n",
      "logdet: 1.1748, prior p: 0.4845\n",
      "\n",
      "\n",
      "epoch 61 lr 0.000644 loss -0.6567\n",
      "layer norms 4.1126 5.7639 4.9301 0.9691\n",
      "logdet: 1.1289, prior p: 0.4846\n",
      "\n",
      "\n",
      "epoch 62 lr 0.000614 loss -0.6530\n",
      "layer norms 4.5196 6.3726 5.3889 1.0003\n",
      "logdet: 1.0770, prior p: 0.5001\n",
      "\n",
      "\n",
      "epoch 63 lr 0.000585 loss -0.6533\n",
      "layer norms 4.0190 5.5686 4.8443 0.9961\n",
      "logdet: 1.1418, prior p: 0.4981\n",
      "\n",
      "\n",
      "epoch 64 lr 0.000557 loss -0.6525\n",
      "layer norms 4.0791 5.6934 4.9934 1.0035\n",
      "logdet: 1.1450, prior p: 0.5017\n",
      "\n",
      "\n",
      "epoch 65 lr 0.000528 loss -0.6556\n",
      "layer norms 4.2084 5.8587 5.0982 1.0395\n",
      "logdet: 1.1349, prior p: 0.5198\n",
      "\n",
      "\n",
      "epoch 66 lr 0.000501 loss -0.6543\n",
      "layer norms 4.2072 5.8712 5.0430 1.0257\n",
      "logdet: 1.1290, prior p: 0.5129\n",
      "\n",
      "\n",
      "epoch 67 lr 0.000474 loss -0.6575\n",
      "layer norms 3.7277 5.1124 4.4579 0.9627\n",
      "logdet: 1.1837, prior p: 0.4814\n",
      "\n",
      "\n",
      "epoch 68 lr 0.000447 loss -0.6481\n",
      "layer norms 4.2170 5.8213 4.9914 1.0010\n",
      "logdet: 1.1179, prior p: 0.5005\n",
      "\n",
      "\n",
      "epoch 69 lr 0.000421 loss -0.6568\n",
      "layer norms 4.2151 5.9873 5.1553 0.9717\n",
      "logdet: 1.1206, prior p: 0.4858\n",
      "sampling complete. Sample mean: 0.2601, std: 0.2213, max: 0.9855, min: 0.0132\n",
      "latent mean: 0.4894, std: 0.2057\n",
      "\n",
      "\n",
      "epoch 70 lr 0.000395 loss -0.6536\n",
      "layer norms 4.2008 5.9297 5.1707 1.0289\n",
      "logdet: 1.1395, prior p: 0.5145\n",
      "\n",
      "\n",
      "epoch 71 lr 0.000370 loss -0.6564\n",
      "layer norms 3.9537 5.4561 4.7796 0.9912\n",
      "logdet: 1.1637, prior p: 0.4956\n",
      "\n",
      "\n",
      "epoch 72 lr 0.000346 loss -0.6558\n",
      "layer norms 3.7746 5.1556 4.5653 0.9649\n",
      "logdet: 1.1892, prior p: 0.4825\n",
      "\n",
      "\n",
      "epoch 73 lr 0.000322 loss -0.6570\n",
      "layer norms 3.9573 5.5264 4.8269 0.9841\n",
      "logdet: 1.1586, prior p: 0.4921\n",
      "\n",
      "\n",
      "epoch 74 lr 0.000299 loss -0.6580\n",
      "layer norms 3.5102 4.7271 4.3363 0.9826\n",
      "logdet: 1.2158, prior p: 0.4913\n",
      "\n",
      "\n",
      "epoch 75 lr 0.000277 loss -0.6577\n",
      "layer norms 4.0610 5.7106 4.9632 0.9745\n",
      "logdet: 1.1436, prior p: 0.4873\n",
      "\n",
      "\n",
      "epoch 76 lr 0.000256 loss -0.6607\n",
      "layer norms 3.4195 4.6434 4.4049 1.0118\n",
      "logdet: 1.2566, prior p: 0.5059\n",
      "\n",
      "\n",
      "epoch 77 lr 0.000235 loss -0.6578\n",
      "layer norms 3.8530 5.3116 4.5982 0.9556\n",
      "logdet: 1.1709, prior p: 0.4778\n",
      "\n",
      "\n",
      "epoch 78 lr 0.000215 loss -0.6543\n",
      "layer norms 3.8176 5.2878 4.6907 0.9843\n",
      "logdet: 1.1839, prior p: 0.4922\n",
      "\n",
      "\n",
      "epoch 79 lr 0.000196 loss -0.6581\n",
      "layer norms 3.5520 4.8390 4.4116 0.9777\n",
      "logdet: 1.2196, prior p: 0.4889\n",
      "sampling complete. Sample mean: 0.2564, std: 0.2250, max: 0.9845, min: 0.0134\n",
      "latent mean: 0.5067, std: 0.2063\n",
      "\n",
      "\n",
      "epoch 80 lr 0.000177 loss -0.6593\n",
      "layer norms 3.8069 5.2511 4.6619 0.9912\n",
      "logdet: 1.1848, prior p: 0.4956\n",
      "\n",
      "\n",
      "epoch 81 lr 0.000160 loss -0.6557\n",
      "layer norms 3.7271 5.1204 4.5743 0.9940\n",
      "logdet: 1.1944, prior p: 0.4970\n",
      "\n",
      "\n",
      "epoch 82 lr 0.000143 loss -0.6561\n",
      "layer norms 3.7140 5.1070 4.5319 0.9844\n",
      "logdet: 1.1914, prior p: 0.4922\n",
      "\n",
      "\n",
      "epoch 83 lr 0.000127 loss -0.6638\n",
      "layer norms 3.9107 5.4048 4.7737 0.9927\n",
      "logdet: 1.1638, prior p: 0.4964\n",
      "\n",
      "\n",
      "epoch 84 lr 0.000112 loss -0.6631\n",
      "layer norms 4.3754 6.1346 5.2780 1.0061\n",
      "logdet: 1.0943, prior p: 0.5031\n",
      "\n",
      "\n",
      "epoch 85 lr 0.000098 loss -0.6616\n",
      "layer norms 3.9710 5.5407 4.8498 0.9884\n",
      "logdet: 1.1504, prior p: 0.4942\n",
      "\n",
      "\n",
      "epoch 86 lr 0.000085 loss -0.6556\n",
      "layer norms 3.5524 4.8495 4.4315 0.9883\n",
      "logdet: 1.2263, prior p: 0.4941\n",
      "\n",
      "\n",
      "epoch 87 lr 0.000073 loss -0.6561\n",
      "layer norms 3.7493 5.1458 4.5334 0.9778\n",
      "logdet: 1.1858, prior p: 0.4889\n",
      "\n",
      "\n",
      "epoch 88 lr 0.000061 loss -0.6664\n",
      "layer norms 3.6775 5.0535 4.6190 1.0056\n",
      "logdet: 1.2090, prior p: 0.5028\n",
      "\n",
      "\n",
      "epoch 89 lr 0.000051 loss -0.6617\n",
      "layer norms 3.8674 5.3566 4.7372 0.9792\n",
      "logdet: 1.1680, prior p: 0.4896\n",
      "sampling complete. Sample mean: 0.2544, std: 0.2243, max: 0.9826, min: 0.0120\n",
      "latent mean: 0.5000, std: 0.2062\n",
      "\n",
      "\n",
      "epoch 90 lr 0.000041 loss -0.6605\n",
      "layer norms 4.0141 5.5470 4.8928 0.9984\n",
      "logdet: 1.1461, prior p: 0.4992\n",
      "\n",
      "\n",
      "epoch 91 lr 0.000033 loss -0.6620\n",
      "layer norms 4.4271 6.2852 5.4446 1.0270\n",
      "logdet: 1.0910, prior p: 0.5135\n",
      "\n",
      "\n",
      "epoch 92 lr 0.000026 loss -0.6604\n",
      "layer norms 4.0937 5.6991 4.9604 0.9946\n",
      "logdet: 1.1325, prior p: 0.4973\n",
      "\n",
      "\n",
      "epoch 93 lr 0.000019 loss -0.6614\n",
      "layer norms 4.0322 5.5926 4.9532 1.0208\n",
      "logdet: 1.1512, prior p: 0.5104\n",
      "\n",
      "\n",
      "epoch 94 lr 0.000014 loss -0.6680\n",
      "layer norms 3.9768 5.5017 4.9138 1.0216\n",
      "logdet: 1.1600, prior p: 0.5108\n",
      "\n",
      "\n",
      "epoch 95 lr 0.000009 loss -0.6619\n",
      "layer norms 4.0308 5.6504 4.8551 0.9619\n",
      "logdet: 1.1406, prior p: 0.4809\n",
      "\n",
      "\n",
      "epoch 96 lr 0.000006 loss -0.6628\n",
      "layer norms 3.7375 5.1292 4.6519 0.9906\n",
      "logdet: 1.1959, prior p: 0.4953\n",
      "\n",
      "\n",
      "epoch 97 lr 0.000003 loss -0.6608\n",
      "layer norms 3.9641 5.5324 4.8141 0.9910\n",
      "logdet: 1.1488, prior p: 0.4955\n",
      "\n",
      "\n",
      "epoch 98 lr 0.000002 loss -0.6595\n",
      "layer norms 3.8548 5.3192 4.7401 1.0008\n",
      "logdet: 1.1726, prior p: 0.5004\n",
      "\n",
      "\n",
      "epoch 99 lr 0.000001 loss -0.6639\n",
      "layer norms 3.8569 5.3388 4.7322 0.9885\n",
      "logdet: 1.1713, prior p: 0.4943\n",
      "sampling complete. Sample mean: 0.2538, std: 0.2236, max: 0.9822, min: 0.0122\n",
      "latent mean: 0.5012, std: 0.2068\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = 'mnist'\n",
    "num_classes = 10\n",
    "img_size = 28\n",
    "channel_size = 1\n",
    "\n",
    "# we use a small model for fast demonstration, increase the model size for better results\n",
    "patch_size = 4\n",
    "channels = 128\n",
    "blocks = 4\n",
    "layers_per_block = 4\n",
    "# try different noise levels to see its effect\n",
    "noise_std = 0.1\n",
    "\n",
    "batch_size = 256\n",
    "lr = 2e-3\n",
    "# increase epochs for better results\n",
    "epochs = 100\n",
    "sample_freq = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda' \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps' # if on mac\n",
    "else:\n",
    "    device = 'cpu' # if mps not available\n",
    "print(f'using device {device}')\n",
    "\n",
    "fixed_noise = torch.randn(num_classes * 10, (img_size // patch_size)**2, channel_size * patch_size ** 2, device=device)\n",
    "fixed_y = torch.arange(num_classes, device=device).view(-1, 1).repeat(1, 10).flatten()\n",
    "\n",
    "transform = tv.transforms.Compose([\n",
    "    tv.transforms.Resize((img_size, img_size)),\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "data = tv.datasets.MNIST('.', transform=transform, train=True, download=True)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "model = Model(in_channels=channel_size, img_size=img_size, patch_size=patch_size, \n",
    "              channels=channels, num_blocks=blocks, layers_per_block=layers_per_block,\n",
    "              num_classes=num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), betas=(0.9, 0.95), lr=lr, weight_decay=1e-4)\n",
    "lr_schedule = utils.CosineLRSchedule(optimizer, len(data_loader), epochs * len(data_loader), 1e-6, lr)\n",
    "\n",
    "model_name = f'original'\n",
    "sample_dir = notebook_output_path / f'{dataset}_samples_{model_name}'\n",
    "ckpt_file = notebook_output_path / f'{dataset}_model_{model_name}.pth'\n",
    "sample_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = 0\n",
    "    for x, y in data_loader:\n",
    "        x = x.to(device)\n",
    "        eps = noise_std * torch.randn_like(x)\n",
    "        x = x + eps\n",
    "        x = rescale(x, 0.0001, 0.9999)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z, outputs, logdets = model(x, y)\n",
    "        loss = model.get_loss(z, logdets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_schedule.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    print(f\"epoch {epoch} lr {optimizer.param_groups[0]['lr']:.6f} loss {losses / len(data_loader):.4f}\")\n",
    "    print('layer norms', ' '.join([f'{z.pow(2).mean():.4f}' for z in outputs]))\n",
    "    print(f'logdet: {logdets.mean():.4f}, prior p: {0.5 * z.pow(2).mean():.4f}')\n",
    "    if (epoch + 1) % sample_freq == 0:\n",
    "        with torch.no_grad():\n",
    "            samples = model.reverse(fixed_noise, fixed_y)\n",
    "        tv.utils.save_image(samples, sample_dir / f'samples_{epoch:03d}.png', normalize=False, nrow=10)\n",
    "        latents = model.unpatchify(z[:100])\n",
    "        tv.utils.save_image(latents, sample_dir / f'latent_{epoch:03d}.png', normalize=False, nrow=10)\n",
    "        print(f'sampling complete. Sample mean: {samples.mean():.4f}, std: {samples.std():.4f}, max: {samples.max():.4f}, min: {samples.min():.4f}')\n",
    "        print(f'latent mean: {latents.mean():.4f}, std: {latents.std():.4f}')\n",
    "    print('\\n')\n",
    "torch.save(model.state_dict(), ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3fcf68-b9e5-4840-a26e-5091a16e98c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Warning! NaN detected in patchify\n",
      "Warning! NaN detected in block 0 logdet\n",
      "Warning! NaN detected in block 0 output\n",
      "Warning! NaN detected in block 1 logdet\n",
      "Warning! NaN detected in block 1 output\n",
      "Warning! NaN detected in block 2 logdet\n",
      "Warning! NaN detected in block 2 output\n",
      "Warning! NaN detected in block 3 logdet\n",
      "Warning! NaN detected in block 3 output\n",
      "Accuracy %9.80\n"
     ]
    }
   ],
   "source": [
    "# now we can also evaluate the model by turning it into a classifier with Bayes rule, p(y|x) = p(y)p(x|y)/p(x)\n",
    "data = tv.datasets.MNIST('.', transform=transform, train=False, download=False)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "num_correct = 0\n",
    "num_examples = 0\n",
    "for x, y in data_loader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    eps = noise_std * torch.randn_like(x)\n",
    "    x = x.repeat(num_classes, 1, 1, 1)\n",
    "    y_ = torch.arange(num_classes, device=device).view(-1, 1).repeat(1, y.size(0)).flatten()\n",
    "    with torch.no_grad():\n",
    "        z, outputs, logdets = model(x, y_)\n",
    "        losses = 0.5 * z.pow(2).mean(dim=[1, 2]) - logdets # keep the batch dimension\n",
    "        pred = losses.reshape(num_classes, y.size(0)).argmin(dim=0)\n",
    "    num_correct += (pred == y).sum()\n",
    "    num_examples += y.size(0)\n",
    "print(f'Accuracy %{100 * num_correct / num_examples:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911a2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# from transformer_flow import Model\n",
    "# import utils\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" # set GPU\n",
    "# utils.set_random_seed(100)\n",
    "\n",
    "# num_classes = 10\n",
    "# img_size = 28\n",
    "# channel_size = 1\n",
    "\n",
    "# # we use a small model for fast demonstration, increase the model size for better results\n",
    "# patch_size = 4\n",
    "# channels = 128\n",
    "# blocks = 4\n",
    "# layers_per_block = 4\n",
    "# # try different noise levels to see its effect\n",
    "# noise_std = 0.1\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = 'cuda' \n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = 'mps' # if on mac\n",
    "# else:\n",
    "#     device = 'cpu' # if mps not available\n",
    "# print(f'using device {device}')\n",
    "\n",
    "# fixed_noise = torch.randn(num_classes * 10, (img_size // patch_size)**2, channel_size * patch_size ** 2, device=device)\n",
    "# # fixed_noise = torch.randn(num_classes * 10, (img_size // patch_size)**2, channels, device=device)\n",
    "# fixed_y = torch.arange(num_classes, device=device).view(-1, 1).repeat(1, 10).flatten()\n",
    "\n",
    "# # load the model\n",
    "# model = Model(in_channels=channel_size, img_size=img_size, patch_size=patch_size, \n",
    "#               channels=channels, num_blocks=blocks, layers_per_block=layers_per_block,\n",
    "#               num_classes=num_classes).to(device)\n",
    "# model.load_state_dict(torch.load(\"runs/notebook/mnist_model_4_128_4_4_0.10_linears_with_residual.pth\"))\n",
    "# model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     samples = model.reverse(fixed_noise, fixed_y)\n",
    "#     # print the mean and std of the samples\n",
    "#     mean = samples.mean(dim=[0, 2, 3])\n",
    "#     std = samples.std(dim=[0, 2, 3])\n",
    "#     print(f'mean: {mean}, std: {std}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
