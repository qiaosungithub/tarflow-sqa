{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413d2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import os\n",
    "from transformer_flow import Model\n",
    "import utils\n",
    "import pathlib\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" # set GPU\n",
    "\n",
    "utils.set_random_seed(100)\n",
    "notebook_output_path = pathlib.Path('runs/notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5c805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n",
      "Unitary logdet: 0.00\n",
      "Unitary logdet: -0.00\n",
      "Unitary logdet: -0.00\n",
      "Unitary logdet: 0.00\n",
      "Number of parameters: 3.24M\n",
      "epoch 0 lr 0.002000 loss -1.0247\n",
      "layer norms 1.8771 1.6380 1.0888 0.9851\n",
      "logdet: 1.8387, prior p: 0.4925\n",
      "\n",
      "\n",
      "epoch 1 lr 0.001999 loss -1.4024\n",
      "layer norms 1.4899 1.5833 1.1247 0.9787\n",
      "logdet: 1.9280, prior p: 0.4894\n",
      "\n",
      "\n",
      "epoch 2 lr 0.001998 loss -1.4833\n",
      "layer norms 1.3956 1.4160 0.9017 1.0102\n",
      "logdet: 2.0300, prior p: 0.5051\n",
      "\n",
      "\n",
      "epoch 3 lr 0.001995 loss -1.5293\n",
      "layer norms 1.3892 1.4143 0.9110 1.0398\n",
      "logdet: 2.0683, prior p: 0.5199\n",
      "\n",
      "\n",
      "epoch 4 lr 0.001992 loss -1.5486\n",
      "layer norms 1.4436 1.5376 1.0159 0.9728\n",
      "logdet: 2.0410, prior p: 0.4864\n",
      "sampling complete. Sample mean: -0.7193, std: 0.6360\n",
      "latent mean: -0.0169, std: 0.9842\n",
      "\n",
      "\n",
      "epoch 5 lr 0.001987 loss -1.5612\n",
      "layer norms 1.4839 1.5976 1.0974 1.0033\n",
      "logdet: 2.0737, prior p: 0.5016\n",
      "\n",
      "\n",
      "epoch 6 lr 0.001982 loss -1.5680\n",
      "layer norms 1.5299 1.6882 1.1936 1.0062\n",
      "logdet: 2.0678, prior p: 0.5031\n",
      "\n",
      "\n",
      "epoch 7 lr 0.001975 loss -1.5737\n",
      "layer norms 1.5754 1.7748 1.3000 1.0049\n",
      "logdet: 2.0731, prior p: 0.5025\n",
      "\n",
      "\n",
      "epoch 8 lr 0.001968 loss -1.5770\n",
      "layer norms 1.5915 1.8128 1.3357 1.0261\n",
      "logdet: 2.0863, prior p: 0.5130\n",
      "\n",
      "\n",
      "epoch 9 lr 0.001960 loss -1.5815\n",
      "layer norms 1.6227 1.9143 1.4577 0.9934\n",
      "logdet: 2.0746, prior p: 0.4967\n",
      "sampling complete. Sample mean: -0.7417, std: 0.6238\n",
      "latent mean: -0.0402, std: 0.9951\n",
      "\n",
      "\n",
      "epoch 10 lr 0.001950 loss -1.5838\n",
      "layer norms 1.6400 1.9495 1.5372 1.0135\n",
      "logdet: 2.1009, prior p: 0.5067\n",
      "\n",
      "\n",
      "epoch 11 lr 0.001940 loss -1.5844\n",
      "layer norms 1.6828 2.0300 1.6098 1.0358\n",
      "logdet: 2.1010, prior p: 0.5179\n",
      "\n",
      "\n",
      "epoch 12 lr 0.001928 loss -1.5883\n",
      "layer norms 1.7103 2.1031 1.7015 1.0194\n",
      "logdet: 2.0996, prior p: 0.5097\n",
      "\n",
      "\n",
      "epoch 13 lr 0.001916 loss -1.5905\n",
      "layer norms 1.7437 2.1814 1.8113 1.0143\n",
      "logdet: 2.1009, prior p: 0.5071\n",
      "\n",
      "\n",
      "epoch 14 lr 0.001903 loss -1.5907\n",
      "layer norms 1.7694 2.2363 1.8613 1.0295\n",
      "logdet: 2.1101, prior p: 0.5147\n",
      "sampling complete. Sample mean: -0.7342, std: 0.6173\n",
      "latent mean: 0.0451, std: 1.0147\n",
      "\n",
      "\n",
      "epoch 15 lr 0.001889 loss -1.5932\n",
      "layer norms 1.7827 2.2864 1.9319 0.9614\n",
      "logdet: 2.0761, prior p: 0.4807\n",
      "\n",
      "\n",
      "epoch 16 lr 0.001874 loss -1.5934\n",
      "layer norms 1.8053 2.3725 2.0200 1.0075\n",
      "logdet: 2.1098, prior p: 0.5038\n",
      "\n",
      "\n",
      "epoch 17 lr 0.001858 loss -1.5959\n",
      "layer norms 1.8390 2.4254 2.1144 1.0023\n",
      "logdet: 2.1004, prior p: 0.5011\n",
      "\n",
      "\n",
      "epoch 18 lr 0.001841 loss -1.5970\n",
      "layer norms 1.8524 2.4569 2.1523 0.9849\n",
      "logdet: 2.0941, prior p: 0.4924\n",
      "\n",
      "\n",
      "epoch 19 lr 0.001824 loss -1.5981\n",
      "layer norms 1.8837 2.5379 2.2493 0.9857\n",
      "logdet: 2.0936, prior p: 0.4929\n",
      "sampling complete. Sample mean: -0.7299, std: 0.6327\n",
      "latent mean: -0.0190, std: 0.9874\n",
      "\n",
      "\n",
      "epoch 20 lr 0.001805 loss -1.5987\n",
      "layer norms 1.9204 2.6086 2.3382 1.0024\n",
      "logdet: 2.1031, prior p: 0.5012\n",
      "\n",
      "\n",
      "epoch 21 lr 0.001786 loss -1.6001\n",
      "layer norms 1.9153 2.6269 2.3850 0.9990\n",
      "logdet: 2.1000, prior p: 0.4995\n",
      "\n",
      "\n",
      "epoch 22 lr 0.001766 loss -1.6007\n",
      "layer norms 1.9676 2.7207 2.4833 1.0168\n",
      "logdet: 2.1107, prior p: 0.5084\n",
      "\n",
      "\n",
      "epoch 23 lr 0.001745 loss -1.6017\n",
      "layer norms 1.9681 2.7272 2.4906 0.9981\n",
      "logdet: 2.1002, prior p: 0.4990\n",
      "\n",
      "\n",
      "epoch 24 lr 0.001724 loss -1.6024\n",
      "layer norms 1.9946 2.7963 2.6093 0.9965\n",
      "logdet: 2.1009, prior p: 0.4982\n",
      "sampling complete. Sample mean: -0.7205, std: 0.6417\n",
      "latent mean: 0.0150, std: 0.9999\n",
      "\n",
      "\n",
      "epoch 25 lr 0.001702 loss -1.6031\n",
      "layer norms 1.9829 2.7893 2.6012 0.9772\n",
      "logdet: 2.0983, prior p: 0.4886\n",
      "\n",
      "\n",
      "epoch 26 lr 0.001679 loss -1.6041\n",
      "layer norms 2.0726 2.9511 2.7867 1.0184\n",
      "logdet: 2.1039, prior p: 0.5092\n",
      "\n",
      "\n",
      "epoch 27 lr 0.001655 loss -1.6040\n",
      "layer norms 2.0405 2.9075 2.7532 1.0105\n",
      "logdet: 2.1091, prior p: 0.5052\n",
      "\n",
      "\n",
      "epoch 28 lr 0.001631 loss -1.6050\n",
      "layer norms 2.0572 2.9318 2.7698 1.0085\n",
      "logdet: 2.1056, prior p: 0.5043\n",
      "\n",
      "\n",
      "epoch 29 lr 0.001606 loss -1.6057\n",
      "layer norms 2.0665 2.9562 2.8196 0.9871\n",
      "logdet: 2.1004, prior p: 0.4935\n",
      "sampling complete. Sample mean: -0.7173, std: 0.6458\n",
      "latent mean: 0.0068, std: 0.9934\n",
      "\n",
      "\n",
      "epoch 30 lr 0.001580 loss -1.6062\n",
      "layer norms 2.1008 3.0329 2.9274 1.0157\n",
      "logdet: 2.1085, prior p: 0.5079\n",
      "\n",
      "\n",
      "epoch 31 lr 0.001554 loss -1.6070\n",
      "layer norms 2.0966 3.0500 2.9685 1.0092\n",
      "logdet: 2.1115, prior p: 0.5046\n",
      "\n",
      "\n",
      "epoch 32 lr 0.001527 loss -1.6077\n",
      "layer norms 2.1156 3.1086 3.0379 1.0016\n",
      "logdet: 2.1099, prior p: 0.5008\n",
      "\n",
      "\n",
      "epoch 33 lr 0.001500 loss -1.6081\n",
      "layer norms 2.1146 3.0943 3.0795 0.9835\n",
      "logdet: 2.1038, prior p: 0.4917\n",
      "\n",
      "\n",
      "epoch 34 lr 0.001473 loss -1.6088\n",
      "layer norms 2.1370 3.1467 3.1122 0.9898\n",
      "logdet: 2.1046, prior p: 0.4949\n",
      "sampling complete. Sample mean: -0.7302, std: 0.6333\n",
      "latent mean: -0.0057, std: 0.9977\n",
      "\n",
      "\n",
      "epoch 35 lr 0.001444 loss -1.6092\n",
      "layer norms 2.1366 3.1441 3.1342 0.9993\n",
      "logdet: 2.1079, prior p: 0.4996\n",
      "\n",
      "\n",
      "epoch 36 lr 0.001416 loss -1.6095\n",
      "layer norms 2.1465 3.1773 3.1968 0.9873\n",
      "logdet: 2.1050, prior p: 0.4936\n",
      "\n",
      "\n",
      "epoch 37 lr 0.001387 loss -1.6102\n",
      "layer norms 2.1442 3.1791 3.2144 0.9969\n",
      "logdet: 2.1097, prior p: 0.4985\n",
      "\n",
      "\n",
      "epoch 38 lr 0.001357 loss -1.6107\n",
      "layer norms 2.1869 3.2703 3.3035 1.0154\n",
      "logdet: 2.1214, prior p: 0.5077\n",
      "\n",
      "\n",
      "epoch 39 lr 0.001327 loss -1.6112\n",
      "layer norms 2.2019 3.3002 3.3485 1.0141\n",
      "logdet: 2.1139, prior p: 0.5070\n",
      "sampling complete. Sample mean: -0.7138, std: 0.6483\n",
      "latent mean: 0.0128, std: 1.0051\n",
      "\n",
      "\n",
      "epoch 40 lr 0.001297 loss -1.6117\n",
      "layer norms 2.2241 3.3433 3.4166 0.9834\n",
      "logdet: 2.0991, prior p: 0.4917\n",
      "\n",
      "\n",
      "epoch 41 lr 0.001267 loss -1.6121\n",
      "layer norms 2.2071 3.3125 3.3946 0.9944\n",
      "logdet: 2.1100, prior p: 0.4972\n",
      "\n",
      "\n",
      "epoch 42 lr 0.001236 loss -1.6124\n",
      "layer norms 2.1987 3.3266 3.4395 1.0037\n",
      "logdet: 2.1135, prior p: 0.5019\n",
      "\n",
      "\n",
      "epoch 43 lr 0.001205 loss -1.6133\n",
      "layer norms 2.2066 3.3771 3.5252 0.9995\n",
      "logdet: 2.1159, prior p: 0.4997\n",
      "\n",
      "\n",
      "epoch 44 lr 0.001174 loss -1.6138\n",
      "layer norms 2.2068 3.3576 3.4834 1.0085\n",
      "logdet: 2.1177, prior p: 0.5042\n",
      "sampling complete. Sample mean: -0.7282, std: 0.6350\n",
      "latent mean: 0.0098, std: 1.0047\n",
      "\n",
      "\n",
      "epoch 45 lr 0.001143 loss -1.6141\n",
      "layer norms 2.2435 3.4256 3.5998 1.0029\n",
      "logdet: 2.1157, prior p: 0.5015\n",
      "\n",
      "\n",
      "epoch 46 lr 0.001111 loss -1.6145\n",
      "layer norms 2.2429 3.4260 3.6236 1.0007\n",
      "logdet: 2.1165, prior p: 0.5003\n",
      "\n",
      "\n",
      "epoch 47 lr 0.001080 loss -1.6150\n",
      "layer norms 2.2075 3.3662 3.5304 0.9989\n",
      "logdet: 2.1143, prior p: 0.4994\n",
      "\n",
      "\n",
      "epoch 48 lr 0.001048 loss -1.6152\n",
      "layer norms 2.2290 3.4058 3.6030 0.9988\n",
      "logdet: 2.1101, prior p: 0.4994\n",
      "\n",
      "\n",
      "epoch 49 lr 0.001016 loss -1.6160\n",
      "layer norms 2.2494 3.4392 3.6431 0.9915\n",
      "logdet: 2.1144, prior p: 0.4958\n",
      "sampling complete. Sample mean: -0.7196, std: 0.6447\n",
      "latent mean: 0.0169, std: 0.9983\n",
      "\n",
      "\n",
      "epoch 50 lr 0.000985 loss -1.6164\n",
      "layer norms 2.2444 3.4343 3.6502 0.9982\n",
      "logdet: 2.1139, prior p: 0.4991\n",
      "\n",
      "\n",
      "epoch 51 lr 0.000953 loss -1.6167\n",
      "layer norms 2.2490 3.4650 3.6891 1.0110\n",
      "logdet: 2.1143, prior p: 0.5055\n",
      "\n",
      "\n",
      "epoch 52 lr 0.000921 loss -1.6173\n",
      "layer norms 2.2275 3.4179 3.6516 1.0108\n",
      "logdet: 2.1214, prior p: 0.5054\n",
      "\n",
      "\n",
      "epoch 53 lr 0.000890 loss -1.6177\n",
      "layer norms 2.2322 3.4417 3.7070 1.0034\n",
      "logdet: 2.1254, prior p: 0.5017\n",
      "\n",
      "\n",
      "epoch 54 lr 0.000858 loss -1.6180\n",
      "layer norms 2.2707 3.5259 3.8051 0.9923\n",
      "logdet: 2.1171, prior p: 0.4962\n",
      "sampling complete. Sample mean: -0.7368, std: 0.6283\n",
      "latent mean: -0.0078, std: 0.9967\n",
      "\n",
      "\n",
      "epoch 55 lr 0.000827 loss -1.6186\n",
      "layer norms 2.2401 3.4881 3.7703 1.0033\n",
      "logdet: 2.1193, prior p: 0.5016\n",
      "\n",
      "\n",
      "epoch 56 lr 0.000796 loss -1.6189\n",
      "layer norms 2.2833 3.5396 3.8663 1.0016\n",
      "logdet: 2.1215, prior p: 0.5008\n",
      "\n",
      "\n",
      "epoch 57 lr 0.000765 loss -1.6193\n",
      "layer norms 2.2482 3.4927 3.7986 0.9999\n",
      "logdet: 2.1195, prior p: 0.5000\n",
      "\n",
      "\n",
      "epoch 58 lr 0.000734 loss -1.6197\n",
      "layer norms 2.2603 3.5330 3.8349 0.9931\n",
      "logdet: 2.1145, prior p: 0.4966\n",
      "\n",
      "\n",
      "epoch 59 lr 0.000704 loss -1.6202\n",
      "layer norms 2.2530 3.4990 3.8055 0.9840\n",
      "logdet: 2.1156, prior p: 0.4920\n",
      "sampling complete. Sample mean: -0.7252, std: 0.6404\n",
      "latent mean: -0.0023, std: 0.9933\n",
      "\n",
      "\n",
      "epoch 60 lr 0.000674 loss -1.6204\n",
      "layer norms 2.2650 3.5362 3.8417 1.0040\n",
      "logdet: 2.1207, prior p: 0.5020\n",
      "\n",
      "\n",
      "epoch 61 lr 0.000644 loss -1.6209\n",
      "layer norms 2.2687 3.5587 3.8931 0.9942\n",
      "logdet: 2.1157, prior p: 0.4971\n",
      "\n",
      "\n",
      "epoch 62 lr 0.000614 loss -1.6213\n",
      "layer norms 2.2761 3.5677 3.9203 0.9975\n",
      "logdet: 2.1163, prior p: 0.4988\n",
      "\n",
      "\n",
      "epoch 63 lr 0.000585 loss -1.6218\n",
      "layer norms 2.2251 3.4672 3.8111 1.0024\n",
      "logdet: 2.1271, prior p: 0.5012\n",
      "\n",
      "\n",
      "epoch 64 lr 0.000557 loss -1.6217\n",
      "layer norms 2.2695 3.5716 3.9279 1.0043\n",
      "logdet: 2.1207, prior p: 0.5021\n",
      "sampling complete. Sample mean: -0.7319, std: 0.6329\n",
      "latent mean: -0.0021, std: 1.0014\n",
      "\n",
      "\n",
      "epoch 65 lr 0.000528 loss -1.6223\n",
      "layer norms 2.2329 3.5026 3.8606 0.9972\n",
      "logdet: 2.1227, prior p: 0.4986\n",
      "\n",
      "\n",
      "epoch 66 lr 0.000501 loss -1.6227\n",
      "layer norms 2.2711 3.5810 3.9838 1.0067\n",
      "logdet: 2.1297, prior p: 0.5034\n",
      "\n",
      "\n",
      "epoch 67 lr 0.000474 loss -1.6233\n",
      "layer norms 2.2482 3.5414 3.8838 1.0090\n",
      "logdet: 2.1274, prior p: 0.5045\n",
      "\n",
      "\n",
      "epoch 68 lr 0.000447 loss -1.6236\n",
      "layer norms 2.2617 3.5797 3.9348 0.9904\n",
      "logdet: 2.1190, prior p: 0.4952\n",
      "\n",
      "\n",
      "epoch 69 lr 0.000421 loss -1.6239\n",
      "layer norms 2.2332 3.5210 3.8728 0.9948\n",
      "logdet: 2.1260, prior p: 0.4974\n",
      "sampling complete. Sample mean: -0.7243, std: 0.6390\n",
      "latent mean: 0.0004, std: 0.9979\n",
      "\n",
      "\n",
      "epoch 70 lr 0.000395 loss -1.6243\n",
      "layer norms 2.2646 3.5654 3.9646 1.0011\n",
      "logdet: 2.1238, prior p: 0.5006\n",
      "\n",
      "\n",
      "epoch 71 lr 0.000370 loss -1.6245\n",
      "layer norms 2.2605 3.5775 3.9437 1.0003\n",
      "logdet: 2.1225, prior p: 0.5002\n",
      "\n",
      "\n",
      "epoch 72 lr 0.000346 loss -1.6250\n",
      "layer norms 2.2731 3.6154 4.0284 1.0037\n",
      "logdet: 2.1257, prior p: 0.5019\n",
      "\n",
      "\n",
      "epoch 73 lr 0.000322 loss -1.6253\n",
      "layer norms 2.2542 3.5808 3.9678 1.0028\n",
      "logdet: 2.1251, prior p: 0.5014\n",
      "\n",
      "\n",
      "epoch 74 lr 0.000299 loss -1.6256\n",
      "layer norms 2.2686 3.6197 4.0102 1.0060\n",
      "logdet: 2.1265, prior p: 0.5030\n",
      "sampling complete. Sample mean: -0.7304, std: 0.6323\n",
      "latent mean: 0.0113, std: 1.0046\n",
      "\n",
      "\n",
      "epoch 75 lr 0.000277 loss -1.6259\n",
      "layer norms 2.2546 3.6044 4.0036 0.9989\n",
      "logdet: 2.1258, prior p: 0.4994\n",
      "\n",
      "\n",
      "epoch 76 lr 0.000256 loss -1.6263\n",
      "layer norms 2.2290 3.5507 3.9356 1.0017\n",
      "logdet: 2.1278, prior p: 0.5008\n",
      "\n",
      "\n",
      "epoch 77 lr 0.000235 loss -1.6264\n",
      "layer norms 2.2533 3.5875 3.9642 1.0050\n",
      "logdet: 2.1220, prior p: 0.5025\n",
      "\n",
      "\n",
      "epoch 78 lr 0.000215 loss -1.6268\n",
      "layer norms 2.2271 3.5456 3.9266 0.9936\n",
      "logdet: 2.1306, prior p: 0.4968\n",
      "\n",
      "\n",
      "epoch 79 lr 0.000196 loss -1.6271\n",
      "layer norms 2.2382 3.5782 3.9798 1.0011\n",
      "logdet: 2.1245, prior p: 0.5006\n",
      "sampling complete. Sample mean: -0.7260, std: 0.6378\n",
      "latent mean: -0.0062, std: 1.0016\n",
      "\n",
      "\n",
      "epoch 80 lr 0.000177 loss -1.6274\n",
      "layer norms 2.2305 3.5542 3.9479 1.0020\n",
      "logdet: 2.1275, prior p: 0.5010\n",
      "\n",
      "\n",
      "epoch 81 lr 0.000160 loss -1.6278\n",
      "layer norms 2.2490 3.5986 4.0040 1.0018\n",
      "logdet: 2.1254, prior p: 0.5009\n",
      "\n",
      "\n",
      "epoch 82 lr 0.000143 loss -1.6278\n",
      "layer norms 2.2695 3.6382 4.0508 0.9975\n",
      "logdet: 2.1248, prior p: 0.4987\n",
      "\n",
      "\n",
      "epoch 83 lr 0.000127 loss -1.6283\n",
      "layer norms 2.1958 3.5140 3.9004 1.0051\n",
      "logdet: 2.1323, prior p: 0.5025\n",
      "\n",
      "\n",
      "epoch 84 lr 0.000112 loss -1.6284\n",
      "layer norms 2.2632 3.6220 4.0399 1.0031\n",
      "logdet: 2.1249, prior p: 0.5016\n",
      "sampling complete. Sample mean: -0.7331, std: 0.6296\n",
      "latent mean: -0.0113, std: 1.0032\n",
      "\n",
      "\n",
      "epoch 85 lr 0.000098 loss -1.6287\n",
      "layer norms 2.2498 3.6141 4.0079 0.9983\n",
      "logdet: 2.1311, prior p: 0.4992\n",
      "\n",
      "\n",
      "epoch 86 lr 0.000085 loss -1.6290\n",
      "layer norms 2.2781 3.6692 4.0757 0.9922\n",
      "logdet: 2.1221, prior p: 0.4961\n",
      "\n",
      "\n",
      "epoch 87 lr 0.000073 loss -1.6291\n",
      "layer norms 2.2217 3.5542 3.9253 1.0021\n",
      "logdet: 2.1274, prior p: 0.5010\n",
      "\n",
      "\n",
      "epoch 88 lr 0.000061 loss -1.6294\n",
      "layer norms 2.2295 3.5694 3.9806 1.0003\n",
      "logdet: 2.1281, prior p: 0.5002\n",
      "\n",
      "\n",
      "epoch 89 lr 0.000051 loss -1.6294\n",
      "layer norms 2.2479 3.6025 4.0321 0.9961\n",
      "logdet: 2.1275, prior p: 0.4981\n",
      "sampling complete. Sample mean: -0.7306, std: 0.6333\n",
      "latent mean: 0.0107, std: 0.9961\n",
      "\n",
      "\n",
      "epoch 90 lr 0.000041 loss -1.6295\n",
      "layer norms 2.2168 3.5645 3.9670 1.0035\n",
      "logdet: 2.1329, prior p: 0.5018\n",
      "\n",
      "\n",
      "epoch 91 lr 0.000033 loss -1.6299\n",
      "layer norms 2.2211 3.5594 3.9738 1.0013\n",
      "logdet: 2.1333, prior p: 0.5007\n",
      "\n",
      "\n",
      "epoch 92 lr 0.000026 loss -1.6299\n",
      "layer norms 2.2302 3.5836 3.9874 1.0056\n",
      "logdet: 2.1332, prior p: 0.5028\n",
      "\n",
      "\n",
      "epoch 93 lr 0.000019 loss -1.6300\n",
      "layer norms 2.2531 3.6205 4.0348 1.0016\n",
      "logdet: 2.1318, prior p: 0.5008\n",
      "\n",
      "\n",
      "epoch 94 lr 0.000014 loss -1.6299\n",
      "layer norms 2.2330 3.5812 3.9897 1.0007\n",
      "logdet: 2.1291, prior p: 0.5003\n",
      "sampling complete. Sample mean: -0.7307, std: 0.6336\n",
      "latent mean: -0.0027, std: 1.0027\n",
      "\n",
      "\n",
      "epoch 95 lr 0.000009 loss -1.6302\n",
      "layer norms 2.2387 3.6135 4.0312 0.9973\n",
      "logdet: 2.1326, prior p: 0.4987\n",
      "\n",
      "\n",
      "epoch 96 lr 0.000006 loss -1.6304\n",
      "layer norms 2.2455 3.6219 4.0143 0.9986\n",
      "logdet: 2.1301, prior p: 0.4993\n",
      "\n",
      "\n",
      "epoch 97 lr 0.000003 loss -1.6303\n",
      "layer norms 2.2633 3.6433 4.0631 1.0036\n",
      "logdet: 2.1270, prior p: 0.5018\n",
      "\n",
      "\n",
      "epoch 98 lr 0.000002 loss -1.6302\n",
      "layer norms 2.2203 3.5671 3.9645 0.9995\n",
      "logdet: 2.1300, prior p: 0.4998\n",
      "\n",
      "\n",
      "epoch 99 lr 0.000001 loss -1.6304\n",
      "layer norms 2.2509 3.6260 4.0260 0.9913\n",
      "logdet: 2.1297, prior p: 0.4957\n",
      "sampling complete. Sample mean: -0.7302, std: 0.6336\n",
      "latent mean: 0.0023, std: 0.9952\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = 'mnist'\n",
    "num_classes = 10\n",
    "img_size = 28\n",
    "channel_size = 1\n",
    "\n",
    "# we use a small model for fast demonstration, increase the model size for better results\n",
    "patch_size = 4\n",
    "channels = 128\n",
    "blocks = 4\n",
    "layers_per_block = 4\n",
    "# try different noise levels to see its effect\n",
    "noise_std = 0.1\n",
    "\n",
    "batch_size = 256\n",
    "lr = 2e-3\n",
    "# increase epochs for better results\n",
    "epochs = 100\n",
    "sample_freq = 5\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda' \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps' # if on mac\n",
    "else:\n",
    "    device = 'cpu' # if mps not available\n",
    "print(f'using device {device}')\n",
    "\n",
    "fixed_noise = torch.randn(num_classes * 10, (img_size // patch_size)**2, channel_size * patch_size ** 2, device=device)\n",
    "# fixed_noise = torch.randn(num_classes * 10, (img_size // patch_size)**2, channels, device=device)\n",
    "fixed_y = torch.arange(num_classes, device=device).view(-1, 1).repeat(1, 10).flatten()\n",
    "\n",
    "transform = tv.transforms.Compose([\n",
    "    tv.transforms.Resize((img_size, img_size)),\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "data = tv.datasets.MNIST('.', transform=transform, train=True, download=True)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "model = Model(in_channels=channel_size, img_size=img_size, patch_size=patch_size, \n",
    "              channels=channels, num_blocks=blocks, layers_per_block=layers_per_block,\n",
    "              num_classes=num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), betas=(0.9, 0.95), lr=lr, weight_decay=1e-4)\n",
    "lr_schedule = utils.CosineLRSchedule(optimizer, len(data_loader), epochs * len(data_loader), 1e-6, lr)\n",
    "\n",
    "model_name = f'lr{lr}_linears_with_residual'\n",
    "sample_dir = notebook_output_path / f'{dataset}_samples_{model_name}'\n",
    "ckpt_file = notebook_output_path / f'{dataset}_model_{model_name}.pth'\n",
    "sample_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = 0\n",
    "    for x, y in data_loader:\n",
    "        x = x.to(device)\n",
    "        eps = noise_std * torch.randn_like(x)\n",
    "        x = x + eps\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z, outputs, logdets = model(x, y)\n",
    "        loss = model.get_loss(z, logdets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_schedule.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    print(f\"epoch {epoch} lr {optimizer.param_groups[0]['lr']:.6f} loss {losses / len(data_loader):.4f}\")\n",
    "    print('layer norms', ' '.join([f'{z.pow(2).mean():.4f}' for z in outputs]))\n",
    "    print(f'logdet: {logdets.mean():.4f}, prior p: {0.5 * z.pow(2).mean():.4f}')\n",
    "    if (epoch + 1) % sample_freq == 0:\n",
    "        with torch.no_grad():\n",
    "            samples = model.reverse(fixed_noise, fixed_y)\n",
    "        tv.utils.save_image(samples, sample_dir / f'samples_{epoch:03d}.png', normalize=True, nrow=10)\n",
    "        latents = model.unpatchify(z[:100])\n",
    "        tv.utils.save_image(latents, sample_dir / f'latent_{epoch:03d}.png', normalize=True, nrow=10)\n",
    "        print(f'sampling complete. Sample mean: {samples.mean():.4f}, std: {samples.std():.4f}')\n",
    "        print(f'latent mean: {latents.mean():.4f}, std: {latents.std():.4f}')\n",
    "    print('\\n')\n",
    "torch.save(model.state_dict(), ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd3fcf68-b9e5-4840-a26e-5091a16e98c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy %98.30\n"
     ]
    }
   ],
   "source": [
    "# now we can also evaluate the model by turning it into a classifier with Bayes rule, p(y|x) = p(y)p(x|y)/p(x)\n",
    "data = tv.datasets.MNIST('.', transform=transform, train=False, download=False)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "num_correct = 0\n",
    "num_examples = 0\n",
    "for x, y in data_loader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    eps = noise_std * torch.randn_like(x)\n",
    "    x = x.repeat(num_classes, 1, 1, 1)\n",
    "    y_ = torch.arange(num_classes, device=device).view(-1, 1).repeat(1, y.size(0)).flatten()\n",
    "    with torch.no_grad():\n",
    "        z, outputs, logdets = model(x, y_)\n",
    "        losses = 0.5 * z.pow(2).mean(dim=[1, 2]) - logdets # keep the batch dimension\n",
    "        pred = losses.reshape(num_classes, y.size(0)).argmin(dim=0)\n",
    "    num_correct += (pred == y).sum()\n",
    "    num_examples += y.size(0)\n",
    "print(f'Accuracy %{100 * num_correct / num_examples:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "564c5005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# from transformer_flow import Model\n",
    "# import utils\n",
    "# import pathlib\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" # set GPU\n",
    "# utils.set_random_seed(100)\n",
    "# notebook_output_path = pathlib.Path('runs/notebook')\n",
    "\n",
    "# num_classes = 10\n",
    "# img_size = 28\n",
    "# channel_size = 1\n",
    "\n",
    "# # we use a small model for fast demonstration, increase the model size for better results\n",
    "# patch_size = 4\n",
    "# channels = 128\n",
    "# blocks = 4\n",
    "# layers_per_block = 4\n",
    "# # try different noise levels to see its effect\n",
    "# noise_std = 0.1\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = 'cuda' \n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = 'mps' # if on mac\n",
    "# else:\n",
    "#     device = 'cpu' # if mps not available\n",
    "# print(f'using device {device}')\n",
    "\n",
    "# fixed_noise = torch.randn(num_classes * 10, (img_size // patch_size)**2, channel_size * patch_size ** 2, device=device)\n",
    "# # fixed_noise = torch.randn(num_classes * 10, (img_size // patch_size)**2, channels, device=device)\n",
    "# fixed_y = torch.arange(num_classes, device=device).view(-1, 1).repeat(1, 10).flatten()\n",
    "\n",
    "# # load the model\n",
    "# model = Model(in_channels=channel_size, img_size=img_size, patch_size=patch_size, \n",
    "#               channels=channels, num_blocks=blocks, layers_per_block=layers_per_block,\n",
    "#               num_classes=num_classes).to(device)\n",
    "# model.load_state_dict(torch.load(\"runs/notebook/mnist_model_4_128_4_4_0.10_linears_with_residual.pth\"))\n",
    "# model.eval()\n",
    "\n",
    "# model_name = f'lr{lr}_linears_with_residual'\n",
    "# sample_dir = notebook_output_path / f'mnist_samples_{model_name}'\n",
    "\n",
    "# # # code for printing the unitaries\n",
    "# # unitaries = model.unitaries\n",
    "# # for i, unitary in enumerate(unitaries):\n",
    "# #     print(f'unitary {i}:')\n",
    "# #     W = unitary.weight\n",
    "# #     # calculate the eigenvalues\n",
    "# #     eigenvalues, eigenvectors = torch.linalg.eig(W)\n",
    "# #     # print the eigenvalues\n",
    "# #     print(f'eigenvalues: {eigenvalues}\\n')\n",
    "# #     # normalize the eigenvectors\n",
    "# #     eigenvectors = eigenvectors / torch.norm(eigenvectors, dim=1, keepdim=True)\n",
    "# #     # print the max value of the eigenvectors\n",
    "# #     max_eigenvector = torch.abs(eigenvectors).max(dim=1).values\n",
    "# #     assert max_eigenvector.ndim == 1\n",
    "# #     print(f'max eigenvector: {max_eigenvector}\\n')\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     samples = model.reverse(fixed_noise, fixed_y)\n",
    "#     # print the mean and std of the samples\n",
    "#     mean = samples.mean(dim=[0, 2, 3])\n",
    "#     std = samples.std(dim=[0, 2, 3])\n",
    "#     print(f'mean: {mean}, std: {std}')\n",
    "    \n",
    "#     import torchvision as tv\n",
    "    \n",
    "#     tv.utils.save_image(samples, sample_dir / f'samples_114.png', normalize=True, nrow=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
