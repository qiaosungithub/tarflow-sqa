{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413d2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import os\n",
    "from transformer_flow import Model\n",
    "import utils\n",
    "import pathlib\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" # set GPU\n",
    "\n",
    "utils.set_random_seed(100)\n",
    "notebook_output_path = pathlib.Path('runs/notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5c805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n",
      "Number of parameters: 3.23M\n",
      "epoch 0 lr 0.000500 loss -1.2519\n",
      "layer norms 0.7683 1.1461 0.7098 0.7955\n",
      "logdet: 2.1460, prior p: 0.3977\n",
      "epoch 1 lr 0.000500 loss -1.9137\n",
      "layer norms 0.7073 0.7623 0.6817 1.0357\n",
      "logdet: 2.5509, prior p: 0.5178\n",
      "epoch 2 lr 0.000499 loss -2.1119\n",
      "layer norms 0.6204 0.6759 0.6670 0.9696\n",
      "logdet: 2.6671, prior p: 0.4848\n",
      "epoch 3 lr 0.000499 loss -2.1971\n",
      "layer norms 0.5766 0.6280 0.6545 0.9932\n",
      "logdet: 2.7352, prior p: 0.4966\n",
      "epoch 4 lr 0.000498 loss -2.2347\n",
      "layer norms 0.6274 0.6766 0.6919 0.9667\n",
      "logdet: 2.6963, prior p: 0.4833\n",
      "sampling complete\n",
      "epoch 5 lr 0.000497 loss -2.2686\n",
      "layer norms 0.5710 0.6360 0.6957 0.9699\n",
      "logdet: 2.7886, prior p: 0.4850\n",
      "epoch 6 lr 0.000495 loss -2.2970\n",
      "layer norms 0.5645 0.6411 0.7010 0.9676\n",
      "logdet: 2.8060, prior p: 0.4838\n",
      "epoch 7 lr 0.000494 loss -2.3163\n",
      "layer norms 0.5597 0.6447 0.6990 1.0219\n",
      "logdet: 2.8386, prior p: 0.5110\n",
      "epoch 8 lr 0.000492 loss -2.3323\n",
      "layer norms 0.5532 0.6364 0.6808 1.0166\n",
      "logdet: 2.8344, prior p: 0.5083\n",
      "epoch 9 lr 0.000490 loss -2.3472\n",
      "layer norms 0.5405 0.6283 0.6744 0.9742\n",
      "logdet: 2.8603, prior p: 0.4871\n",
      "sampling complete\n",
      "epoch 10 lr 0.000488 loss -2.3596\n",
      "layer norms 0.5239 0.6097 0.6456 0.9644\n",
      "logdet: 2.8597, prior p: 0.4822\n",
      "epoch 11 lr 0.000485 loss -2.3690\n",
      "layer norms 0.5221 0.6150 0.6553 1.0246\n",
      "logdet: 2.8520, prior p: 0.5123\n",
      "epoch 12 lr 0.000482 loss -2.3801\n",
      "layer norms 0.5129 0.6005 0.6308 0.9446\n",
      "logdet: 2.8599, prior p: 0.4723\n",
      "epoch 13 lr 0.000479 loss -2.3884\n",
      "layer norms 0.5208 0.6114 0.6489 0.9905\n",
      "logdet: 2.8790, prior p: 0.4953\n",
      "epoch 14 lr 0.000476 loss -2.3962\n",
      "layer norms 0.5135 0.5980 0.6179 0.9907\n",
      "logdet: 2.8727, prior p: 0.4954\n",
      "sampling complete\n",
      "epoch 15 lr 0.000472 loss -2.4062\n",
      "layer norms 0.5092 0.5925 0.6190 1.0095\n",
      "logdet: 2.9196, prior p: 0.5048\n",
      "epoch 16 lr 0.000469 loss -2.4138\n",
      "layer norms 0.5074 0.5901 0.6123 0.9781\n",
      "logdet: 2.9080, prior p: 0.4890\n",
      "epoch 17 lr 0.000465 loss -2.4222\n",
      "layer norms 0.5028 0.5838 0.6008 0.9724\n",
      "logdet: 2.9152, prior p: 0.4862\n",
      "epoch 18 lr 0.000460 loss -2.4303\n",
      "layer norms 0.5019 0.5813 0.5950 1.0026\n",
      "logdet: 2.9239, prior p: 0.5013\n",
      "epoch 19 lr 0.000456 loss -2.4352\n",
      "layer norms 0.5024 0.5843 0.6019 1.0305\n",
      "logdet: 2.9572, prior p: 0.5153\n",
      "sampling complete\n",
      "epoch 20 lr 0.000451 loss -2.4415\n",
      "layer norms 0.5031 0.5921 0.6005 0.9872\n",
      "logdet: 2.9330, prior p: 0.4936\n",
      "epoch 21 lr 0.000447 loss -2.4465\n",
      "layer norms 0.5075 0.5999 0.6149 0.9781\n",
      "logdet: 2.9387, prior p: 0.4890\n",
      "epoch 22 lr 0.000442 loss -2.4507\n",
      "layer norms 0.5092 0.6008 0.6172 1.0048\n",
      "logdet: 2.9565, prior p: 0.5024\n",
      "epoch 23 lr 0.000436 loss -2.4552\n",
      "layer norms 0.5030 0.5905 0.6088 1.0043\n",
      "logdet: 2.9617, prior p: 0.5022\n",
      "epoch 24 lr 0.000431 loss -2.4587\n",
      "layer norms 0.4999 0.5886 0.6086 0.9767\n",
      "logdet: 2.9519, prior p: 0.4883\n",
      "sampling complete\n",
      "epoch 25 lr 0.000426 loss -2.4618\n",
      "layer norms 0.5059 0.5885 0.6052 0.9741\n",
      "logdet: 2.9556, prior p: 0.4870\n",
      "epoch 26 lr 0.000420 loss -2.4640\n",
      "layer norms 0.5019 0.5914 0.6097 0.9990\n",
      "logdet: 2.9635, prior p: 0.4995\n",
      "epoch 27 lr 0.000414 loss -2.4678\n",
      "layer norms 0.5007 0.5865 0.6042 0.9772\n",
      "logdet: 2.9627, prior p: 0.4886\n",
      "epoch 28 lr 0.000408 loss -2.4690\n",
      "layer norms 0.5117 0.6108 0.6273 0.9920\n",
      "logdet: 2.9707, prior p: 0.4960\n",
      "epoch 29 lr 0.000402 loss -2.4727\n",
      "layer norms 0.4958 0.5852 0.6019 1.0150\n",
      "logdet: 2.9813, prior p: 0.5075\n",
      "sampling complete\n",
      "epoch 30 lr 0.000395 loss -2.4751\n",
      "layer norms 0.5054 0.5966 0.6141 1.0026\n",
      "logdet: 2.9798, prior p: 0.5013\n",
      "epoch 31 lr 0.000389 loss -2.4766\n",
      "layer norms 0.5048 0.6031 0.6215 0.9922\n",
      "logdet: 2.9811, prior p: 0.4961\n",
      "epoch 32 lr 0.000382 loss -2.4794\n",
      "layer norms 0.5070 0.6103 0.6300 1.0223\n",
      "logdet: 2.9864, prior p: 0.5112\n",
      "epoch 33 lr 0.000375 loss -2.4812\n",
      "layer norms 0.5072 0.6130 0.6364 0.9874\n",
      "logdet: 2.9642, prior p: 0.4937\n",
      "epoch 34 lr 0.000368 loss -2.4831\n",
      "layer norms 0.5053 0.6036 0.6196 1.0092\n",
      "logdet: 2.9936, prior p: 0.5046\n",
      "sampling complete\n",
      "epoch 35 lr 0.000361 loss -2.4847\n",
      "layer norms 0.5102 0.6086 0.6241 1.0038\n",
      "logdet: 2.9917, prior p: 0.5019\n",
      "epoch 36 lr 0.000354 loss -2.4871\n",
      "layer norms 0.5028 0.5970 0.6112 0.9570\n",
      "logdet: 2.9659, prior p: 0.4785\n",
      "epoch 37 lr 0.000347 loss -2.4884\n",
      "layer norms 0.5035 0.5998 0.6189 0.9870\n",
      "logdet: 2.9832, prior p: 0.4935\n",
      "epoch 38 lr 0.000340 loss -2.4903\n",
      "layer norms 0.5088 0.6111 0.6355 1.0275\n",
      "logdet: 2.9987, prior p: 0.5137\n",
      "epoch 39 lr 0.000332 loss -2.4918\n",
      "layer norms 0.5084 0.6102 0.6304 1.0000\n",
      "logdet: 2.9890, prior p: 0.5000\n",
      "sampling complete\n",
      "epoch 40 lr 0.000325 loss -2.4935\n",
      "layer norms 0.5030 0.6026 0.6213 0.9942\n",
      "logdet: 2.9938, prior p: 0.4971\n",
      "epoch 41 lr 0.000317 loss -2.4949\n",
      "layer norms 0.5043 0.6038 0.6253 0.9891\n",
      "logdet: 2.9880, prior p: 0.4946\n",
      "epoch 42 lr 0.000309 loss -2.4960\n",
      "layer norms 0.5057 0.6115 0.6376 1.0003\n",
      "logdet: 3.0013, prior p: 0.5002\n",
      "epoch 43 lr 0.000302 loss -2.4976\n",
      "layer norms 0.5074 0.6114 0.6361 0.9976\n",
      "logdet: 2.9989, prior p: 0.4988\n",
      "epoch 44 lr 0.000294 loss -2.4991\n",
      "layer norms 0.5071 0.6113 0.6382 1.0212\n",
      "logdet: 3.0088, prior p: 0.5106\n",
      "sampling complete\n",
      "epoch 45 lr 0.000286 loss -2.5002\n",
      "layer norms 0.5101 0.6178 0.6475 1.0150\n",
      "logdet: 3.0120, prior p: 0.5075\n",
      "epoch 46 lr 0.000278 loss -2.5016\n",
      "layer norms 0.5076 0.6145 0.6381 1.0078\n",
      "logdet: 3.0137, prior p: 0.5039\n",
      "epoch 47 lr 0.000270 loss -2.5027\n",
      "layer norms 0.5107 0.6224 0.6513 1.0025\n",
      "logdet: 3.0044, prior p: 0.5012\n",
      "epoch 48 lr 0.000262 loss -2.5034\n",
      "layer norms 0.5116 0.6204 0.6531 0.9956\n",
      "logdet: 3.0062, prior p: 0.4978\n",
      "epoch 49 lr 0.000254 loss -2.5050\n",
      "layer norms 0.5071 0.6136 0.6398 0.9903\n",
      "logdet: 3.0057, prior p: 0.4952\n",
      "sampling complete\n",
      "epoch 50 lr 0.000247 loss -2.5061\n",
      "layer norms 0.5112 0.6178 0.6494 0.9922\n",
      "logdet: 3.0017, prior p: 0.4961\n",
      "epoch 51 lr 0.000239 loss -2.5072\n",
      "layer norms 0.5071 0.6164 0.6478 1.0063\n",
      "logdet: 3.0122, prior p: 0.5032\n",
      "epoch 52 lr 0.000231 loss -2.5087\n",
      "layer norms 0.5085 0.6169 0.6466 1.0125\n",
      "logdet: 3.0179, prior p: 0.5063\n",
      "epoch 53 lr 0.000223 loss -2.5095\n",
      "layer norms 0.5115 0.6247 0.6555 1.0018\n",
      "logdet: 3.0093, prior p: 0.5009\n",
      "epoch 54 lr 0.000215 loss -2.5103\n",
      "layer norms 0.5122 0.6286 0.6592 0.9965\n",
      "logdet: 3.0135, prior p: 0.4982\n",
      "sampling complete\n",
      "epoch 55 lr 0.000207 loss -2.5110\n",
      "layer norms 0.5107 0.6214 0.6541 0.9905\n",
      "logdet: 3.0136, prior p: 0.4953\n",
      "epoch 56 lr 0.000199 loss -2.5122\n",
      "layer norms 0.5047 0.6184 0.6465 0.9910\n",
      "logdet: 3.0148, prior p: 0.4955\n",
      "epoch 57 lr 0.000192 loss -2.5131\n",
      "layer norms 0.5098 0.6240 0.6595 1.0129\n",
      "logdet: 3.0184, prior p: 0.5064\n",
      "epoch 58 lr 0.000184 loss -2.5139\n",
      "layer norms 0.5067 0.6232 0.6584 1.0060\n",
      "logdet: 3.0195, prior p: 0.5030\n",
      "epoch 59 lr 0.000176 loss -2.5148\n",
      "layer norms 0.5038 0.6157 0.6495 1.0013\n",
      "logdet: 3.0227, prior p: 0.5007\n",
      "sampling complete\n",
      "epoch 60 lr 0.000169 loss -2.5155\n",
      "layer norms 0.5072 0.6230 0.6592 0.9976\n",
      "logdet: 3.0181, prior p: 0.4988\n",
      "epoch 61 lr 0.000161 loss -2.5164\n",
      "layer norms 0.5133 0.6295 0.6642 0.9746\n",
      "logdet: 3.0101, prior p: 0.4873\n",
      "epoch 62 lr 0.000154 loss -2.5173\n",
      "layer norms 0.5119 0.6281 0.6672 1.0050\n",
      "logdet: 3.0179, prior p: 0.5025\n",
      "epoch 63 lr 0.000147 loss -2.5178\n",
      "layer norms 0.5073 0.6237 0.6628 1.0008\n",
      "logdet: 3.0174, prior p: 0.5004\n",
      "epoch 64 lr 0.000140 loss -2.5185\n",
      "layer norms 0.5106 0.6296 0.6675 0.9911\n",
      "logdet: 3.0151, prior p: 0.4955\n",
      "sampling complete\n",
      "epoch 65 lr 0.000133 loss -2.5192\n",
      "layer norms 0.5032 0.6156 0.6517 1.0013\n",
      "logdet: 3.0194, prior p: 0.5007\n",
      "epoch 66 lr 0.000126 loss -2.5199\n",
      "layer norms 0.5030 0.6203 0.6578 0.9975\n",
      "logdet: 3.0234, prior p: 0.4988\n",
      "epoch 67 lr 0.000119 loss -2.5206\n",
      "layer norms 0.5098 0.6337 0.6769 1.0104\n",
      "logdet: 3.0207, prior p: 0.5052\n",
      "epoch 68 lr 0.000112 loss -2.5215\n",
      "layer norms 0.5103 0.6357 0.6817 0.9929\n",
      "logdet: 3.0208, prior p: 0.4965\n",
      "epoch 69 lr 0.000106 loss -2.5219\n",
      "layer norms 0.5059 0.6229 0.6594 1.0015\n",
      "logdet: 3.0215, prior p: 0.5008\n",
      "sampling complete\n",
      "epoch 70 lr 0.000099 loss -2.5225\n",
      "layer norms 0.5031 0.6182 0.6520 1.0109\n",
      "logdet: 3.0301, prior p: 0.5054\n",
      "epoch 71 lr 0.000093 loss -2.5229\n",
      "layer norms 0.5051 0.6265 0.6661 0.9988\n",
      "logdet: 3.0212, prior p: 0.4994\n",
      "epoch 72 lr 0.000087 loss -2.5236\n",
      "layer norms 0.5035 0.6261 0.6641 0.9874\n",
      "logdet: 3.0192, prior p: 0.4937\n",
      "epoch 73 lr 0.000081 loss -2.5242\n",
      "layer norms 0.5008 0.6212 0.6622 0.9987\n",
      "logdet: 3.0198, prior p: 0.4994\n",
      "epoch 74 lr 0.000075 loss -2.5246\n",
      "layer norms 0.5026 0.6251 0.6676 1.0178\n",
      "logdet: 3.0332, prior p: 0.5089\n",
      "sampling complete\n",
      "epoch 75 lr 0.000070 loss -2.5252\n",
      "layer norms 0.5042 0.6267 0.6692 0.9908\n",
      "logdet: 3.0196, prior p: 0.4954\n",
      "epoch 76 lr 0.000065 loss -2.5258\n",
      "layer norms 0.5007 0.6209 0.6604 0.9994\n",
      "logdet: 3.0280, prior p: 0.4997\n",
      "epoch 77 lr 0.000059 loss -2.5258\n",
      "layer norms 0.5037 0.6272 0.6739 1.0066\n",
      "logdet: 3.0212, prior p: 0.5033\n",
      "epoch 78 lr 0.000054 loss -2.5265\n",
      "layer norms 0.5078 0.6277 0.6660 0.9890\n",
      "logdet: 3.0297, prior p: 0.4945\n",
      "epoch 79 lr 0.000050 loss -2.5270\n",
      "layer norms 0.5020 0.6255 0.6701 1.0030\n",
      "logdet: 3.0293, prior p: 0.5015\n",
      "sampling complete\n",
      "epoch 80 lr 0.000045 loss -2.5272\n",
      "layer norms 0.5054 0.6338 0.6831 0.9973\n",
      "logdet: 3.0261, prior p: 0.4986\n",
      "epoch 81 lr 0.000041 loss -2.5278\n",
      "layer norms 0.5007 0.6268 0.6728 1.0007\n",
      "logdet: 3.0313, prior p: 0.5004\n",
      "epoch 82 lr 0.000036 loss -2.5281\n",
      "layer norms 0.5050 0.6302 0.6771 1.0058\n",
      "logdet: 3.0324, prior p: 0.5029\n",
      "epoch 83 lr 0.000032 loss -2.5283\n",
      "layer norms 0.4989 0.6249 0.6728 1.0017\n",
      "logdet: 3.0280, prior p: 0.5009\n",
      "epoch 84 lr 0.000029 loss -2.5287\n",
      "layer norms 0.5004 0.6257 0.6703 1.0013\n",
      "logdet: 3.0279, prior p: 0.5007\n",
      "sampling complete\n",
      "epoch 85 lr 0.000025 loss -2.5291\n",
      "layer norms 0.4973 0.6240 0.6709 0.9963\n",
      "logdet: 3.0315, prior p: 0.4982\n",
      "epoch 86 lr 0.000022 loss -2.5293\n",
      "layer norms 0.5031 0.6306 0.6771 0.9965\n",
      "logdet: 3.0289, prior p: 0.4983\n",
      "epoch 87 lr 0.000019 loss -2.5296\n",
      "layer norms 0.5039 0.6337 0.6839 1.0028\n",
      "logdet: 3.0320, prior p: 0.5014\n",
      "epoch 88 lr 0.000016 loss -2.5300\n",
      "layer norms 0.4999 0.6263 0.6733 1.0040\n",
      "logdet: 3.0307, prior p: 0.5020\n",
      "epoch 89 lr 0.000013 loss -2.5300\n",
      "layer norms 0.4964 0.6232 0.6714 0.9943\n",
      "logdet: 3.0253, prior p: 0.4972\n",
      "sampling complete\n",
      "epoch 90 lr 0.000011 loss -2.5301\n",
      "layer norms 0.4996 0.6293 0.6747 0.9997\n",
      "logdet: 3.0307, prior p: 0.4998\n",
      "epoch 91 lr 0.000009 loss -2.5304\n",
      "layer norms 0.4958 0.6229 0.6668 1.0017\n",
      "logdet: 3.0341, prior p: 0.5009\n",
      "epoch 92 lr 0.000007 loss -2.5306\n",
      "layer norms 0.5017 0.6324 0.6807 1.0072\n",
      "logdet: 3.0317, prior p: 0.5036\n",
      "epoch 93 lr 0.000006 loss -2.5308\n",
      "layer norms 0.4976 0.6267 0.6745 0.9996\n",
      "logdet: 3.0286, prior p: 0.4998\n",
      "epoch 94 lr 0.000004 loss -2.5308\n",
      "layer norms 0.4983 0.6222 0.6669 1.0003\n",
      "logdet: 3.0390, prior p: 0.5002\n",
      "sampling complete\n",
      "epoch 95 lr 0.000003 loss -2.5311\n",
      "layer norms 0.5032 0.6334 0.6834 1.0020\n",
      "logdet: 3.0305, prior p: 0.5010\n",
      "epoch 96 lr 0.000002 loss -2.5312\n",
      "layer norms 0.4944 0.6212 0.6666 1.0007\n",
      "logdet: 3.0325, prior p: 0.5004\n",
      "epoch 97 lr 0.000002 loss -2.5312\n",
      "layer norms 0.4940 0.6171 0.6606 1.0022\n",
      "logdet: 3.0355, prior p: 0.5011\n",
      "epoch 98 lr 0.000001 loss -2.5313\n",
      "layer norms 0.4949 0.6212 0.6677 0.9978\n",
      "logdet: 3.0319, prior p: 0.4989\n",
      "epoch 99 lr 0.000001 loss -2.5314\n",
      "layer norms 0.4979 0.6276 0.6786 1.0011\n",
      "logdet: 3.0272, prior p: 0.5005\n",
      "sampling complete\n"
     ]
    }
   ],
   "source": [
    "dataset = 'mnist'\n",
    "num_classes = 10\n",
    "img_size = 28\n",
    "channel_size = 1\n",
    "\n",
    "# we use a small model for fast demonstration, increase the model size for better results\n",
    "patch_size = 4\n",
    "channels = 128\n",
    "blocks = 4\n",
    "layers_per_block = 4\n",
    "# try different noise levels to see its effect\n",
    "noise_std = 0.1\n",
    "\n",
    "batch_size = 256\n",
    "lr = 5e-4\n",
    "# increase epochs for better results\n",
    "epochs = 100\n",
    "sample_freq = 5\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda' \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps' # if on mac\n",
    "else:\n",
    "    device = 'cpu' # if mps not available\n",
    "print(f'using device {device}')\n",
    "\n",
    "fixed_noise = torch.randn(num_classes * 10, (img_size // patch_size)**2, channel_size * patch_size ** 2, device=device)\n",
    "# fixed_noise = torch.randn(num_classes * 10, (img_size // patch_size)**2, channels, device=device)\n",
    "fixed_y = torch.arange(num_classes, device=device).view(-1, 1).repeat(1, 10).flatten()\n",
    "\n",
    "transform = tv.transforms.Compose([\n",
    "    tv.transforms.Resize((img_size, img_size)),\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "data = tv.datasets.MNIST('.', transform=transform, train=True, download=True)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "model = Model(in_channels=channel_size, img_size=img_size, patch_size=patch_size, \n",
    "              channels=channels, num_blocks=blocks, layers_per_block=layers_per_block,\n",
    "              num_classes=num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), betas=(0.9, 0.95), lr=lr, weight_decay=1e-4)\n",
    "lr_schedule = utils.CosineLRSchedule(optimizer, len(data_loader), epochs * len(data_loader), 1e-6, lr)\n",
    "\n",
    "model_name = f'{patch_size}_{channels}_{blocks}_{layers_per_block}_{noise_std:.2f}_sqa_xibo_diaoda'\n",
    "sample_dir = notebook_output_path / f'{dataset}_samples_{model_name}'\n",
    "ckpt_file = notebook_output_path / f'{dataset}_model_{model_name}.pth'\n",
    "sample_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "all_S = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = 0\n",
    "    for x, y in data_loader:\n",
    "        x = x.to(device)\n",
    "        eps = noise_std * torch.randn_like(x)\n",
    "        x = x + eps\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z, outputs, logdets = model(x, y)\n",
    "        loss = model.get_loss(z, logdets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_schedule.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "        # print the singular values of the matrix of the x embedder\n",
    "        # try:\n",
    "        #   W = model.x_embedder.weight\n",
    "        #   S = torch.linalg.svdvals(W, driver='gesvd')\n",
    "        #   all_S.append(S)\n",
    "        # except:\n",
    "        #     print(f\"there is error in calculating the singular values\")\n",
    "        #     print(f\"matrix: {W.shape}\")\n",
    "        #     print(f\"matrix: {W}\")\n",
    "        #     print(f\"loss: {loss}\")\n",
    "        #     loss=torch.tensor(float('nan'), device=device)\n",
    "        # if torch.isnan(loss): \n",
    "            # for i in range(1,6):\n",
    "            #     print(f'singular value of S[-{i}]: {all_S[-i]}')\n",
    "            # assert False\n",
    "          \n",
    "        # print(f'singular values {S.tolist()}')\n",
    "\n",
    "    print(f\"epoch {epoch} lr {optimizer.param_groups[0]['lr']:.6f} loss {losses / len(data_loader):.4f}\")\n",
    "    print('layer norms', ' '.join([f'{z.pow(2).mean():.4f}' for z in outputs]))\n",
    "    print(f'logdet: {logdets.mean():.4f}, prior p: {0.5 * z.pow(2).mean():.4f}')\n",
    "    if (epoch + 1) % sample_freq == 0:\n",
    "        with torch.no_grad():\n",
    "            samples = model.reverse(fixed_noise, fixed_y)\n",
    "        tv.utils.save_image(samples, sample_dir / f'samples_{epoch:03d}.png', normalize=True, nrow=10)\n",
    "        tv.utils.save_image(model.unpatchify(z[:100]), sample_dir / f'latent_{epoch:03d}.png', normalize=True, nrow=10)\n",
    "        print('sampling complete')\n",
    "torch.save(model.state_dict(), ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd3fcf68-b9e5-4840-a26e-5091a16e98c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy %97.11\n"
     ]
    }
   ],
   "source": [
    "# now we can also evaluate the model by turning it into a classifier with Bayes rule, p(y|x) = p(y)p(x|y)/p(x)\n",
    "data = tv.datasets.MNIST('.', transform=transform, train=False, download=False)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "num_correct = 0\n",
    "num_examples = 0\n",
    "for x, y in data_loader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    eps = noise_std * torch.randn_like(x)\n",
    "    x = x.repeat(num_classes, 1, 1, 1)\n",
    "    y_ = torch.arange(num_classes, device=device).view(-1, 1).repeat(1, y.size(0)).flatten()\n",
    "    with torch.no_grad():\n",
    "        z, outputs, logdets = model(x, y_)\n",
    "        losses = 0.5 * z.pow(2).mean(dim=[1, 2]) - logdets # keep the batch dimension\n",
    "        pred = losses.reshape(num_classes, y.size(0)).argmin(dim=0)\n",
    "    num_correct += (pred == y).sum()\n",
    "    num_examples += y.size(0)\n",
    "print(f'Accuracy %{100 * num_correct / num_examples:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
