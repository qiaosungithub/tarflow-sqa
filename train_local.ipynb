{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import os\n",
    "from transformer_flow import Model\n",
    "import utils\n",
    "import pathlib\n",
    "from utils import sqa_save\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" # set GPU\n",
    "\n",
    "utils.set_random_seed(100)\n",
    "notebook_output_path = pathlib.Path('runs/notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n",
      "Number of parameters: 3.23M\n",
      "epoch 0 lr 0.002000 loss -0.9815\n",
      "layer norms 2.4517 2.1962 1.3284 0.9695\n",
      "logdet: 1.8086, prior p: 0.4848\n",
      "\n",
      "\n",
      "epoch 1 lr 0.001999 loss -1.3908\n",
      "layer norms 1.6706 1.7685 1.1945 0.9881\n",
      "logdet: 1.9462, prior p: 0.4941\n",
      "\n",
      "\n",
      "epoch 2 lr 0.001998 loss -1.4849\n",
      "layer norms 1.4575 1.5999 1.0956 0.9269\n",
      "logdet: 1.9591, prior p: 0.4635\n",
      "\n",
      "\n",
      "epoch 3 lr 0.001995 loss -1.5299\n",
      "layer norms 1.3793 1.5176 1.1001 0.8858\n",
      "logdet: 1.9665, prior p: 0.4429\n",
      "\n",
      "\n",
      "epoch 4 lr 0.001992 loss -1.5499\n",
      "layer norms 1.4118 1.5686 1.1717 1.0141\n",
      "logdet: 2.0670, prior p: 0.5070\n",
      "\n",
      "\n",
      "epoch 5 lr 0.001987 loss -1.5611\n",
      "layer norms 1.4471 1.6456 1.2761 0.9989\n",
      "logdet: 2.0615, prior p: 0.4995\n",
      "\n",
      "\n",
      "epoch 6 lr 0.001982 loss -1.5674\n",
      "layer norms 1.4585 1.6693 1.2841 1.0002\n",
      "logdet: 2.0718, prior p: 0.5001\n",
      "\n",
      "\n",
      "epoch 7 lr 0.001975 loss -1.5750\n",
      "layer norms 1.4498 1.7085 1.3997 0.9810\n",
      "logdet: 2.0764, prior p: 0.4905\n",
      "\n",
      "\n",
      "epoch 8 lr 0.001968 loss -1.5791\n",
      "layer norms 1.5136 1.8483 1.5583 0.9470\n",
      "logdet: 2.0360, prior p: 0.4735\n",
      "\n",
      "\n",
      "epoch 9 lr 0.001960 loss -1.5798\n",
      "layer norms 1.5424 1.9259 1.6475 1.0127\n",
      "logdet: 2.0892, prior p: 0.5063\n",
      "sampling complete. Sample mean: -0.7346, std: 0.6279, max: 1.7414, min: -1.7505\n",
      "latent mean: 0.0344, std: 1.0072\n",
      "\n",
      "\n",
      "epoch 10 lr 0.001950 loss -1.5849\n",
      "layer norms 1.5616 1.9888 1.7141 0.9766\n",
      "logdet: 2.0748, prior p: 0.4883\n",
      "\n",
      "\n",
      "epoch 11 lr 0.001940 loss -1.5874\n",
      "layer norms 1.6073 2.0753 1.8488 1.0180\n",
      "logdet: 2.0938, prior p: 0.5090\n",
      "\n",
      "\n",
      "epoch 12 lr 0.001928 loss -1.5844\n",
      "layer norms 1.6685 2.2232 1.9293 0.9871\n",
      "logdet: 2.0805, prior p: 0.4935\n",
      "\n",
      "\n",
      "epoch 13 lr 0.001916 loss -1.5916\n",
      "layer norms 1.6444 2.2135 2.0357 1.0023\n",
      "logdet: 2.0973, prior p: 0.5011\n",
      "\n",
      "\n",
      "epoch 14 lr 0.001903 loss -1.5937\n",
      "layer norms 1.6760 2.2958 2.0991 1.0199\n",
      "logdet: 2.1066, prior p: 0.5099\n",
      "\n",
      "\n",
      "epoch 15 lr 0.001889 loss -1.5952\n",
      "layer norms 1.6828 2.3583 2.2072 0.9924\n",
      "logdet: 2.0945, prior p: 0.4962\n",
      "\n",
      "\n",
      "epoch 16 lr 0.001874 loss -1.5951\n",
      "layer norms 1.7147 2.4358 2.2974 0.9946\n",
      "logdet: 2.1021, prior p: 0.4973\n",
      "\n",
      "\n",
      "epoch 17 lr 0.001858 loss -1.5975\n",
      "layer norms 1.6980 2.4138 2.2785 0.9577\n",
      "logdet: 2.0777, prior p: 0.4788\n",
      "\n",
      "\n",
      "epoch 18 lr 0.001841 loss -1.5930\n",
      "layer norms 1.8211 2.6459 2.4752 1.0017\n",
      "logdet: 2.1012, prior p: 0.5009\n",
      "\n",
      "\n",
      "epoch 19 lr 0.001824 loss -1.5995\n",
      "layer norms 1.8141 2.6723 2.5308 1.0229\n",
      "logdet: 2.0996, prior p: 0.5115\n",
      "sampling complete. Sample mean: -0.7399, std: 0.6220, max: 1.6845, min: -1.6707\n",
      "latent mean: -0.0192, std: 1.0099\n",
      "\n",
      "\n",
      "epoch 20 lr 0.001805 loss -1.6000\n",
      "layer norms 1.8127 2.7256 2.6549 1.0019\n",
      "logdet: 2.0992, prior p: 0.5009\n",
      "\n",
      "\n",
      "epoch 21 lr 0.001786 loss -1.6020\n",
      "layer norms 1.7992 2.7141 2.6629 1.0005\n",
      "logdet: 2.1060, prior p: 0.5003\n",
      "\n",
      "\n",
      "epoch 22 lr 0.001766 loss -1.6027\n",
      "layer norms 1.8374 2.8221 2.7947 1.0162\n",
      "logdet: 2.1101, prior p: 0.5081\n",
      "\n",
      "\n",
      "epoch 23 lr 0.001745 loss -1.6036\n",
      "layer norms 1.8363 2.8562 2.8470 1.0054\n",
      "logdet: 2.1061, prior p: 0.5027\n",
      "\n",
      "\n",
      "epoch 24 lr 0.001724 loss -1.6031\n",
      "layer norms 1.8699 2.8976 2.8754 0.9792\n",
      "logdet: 2.0934, prior p: 0.4896\n",
      "\n",
      "\n",
      "epoch 25 lr 0.001702 loss -1.6052\n",
      "layer norms 1.8518 2.9063 2.9341 0.9768\n",
      "logdet: 2.0973, prior p: 0.4884\n",
      "\n",
      "\n",
      "epoch 26 lr 0.001679 loss -1.6057\n",
      "layer norms 1.8758 2.9727 3.0225 1.0051\n",
      "logdet: 2.1018, prior p: 0.5025\n",
      "\n",
      "\n",
      "epoch 27 lr 0.001655 loss -1.6063\n",
      "layer norms 1.8765 3.0119 3.0778 1.0215\n",
      "logdet: 2.1166, prior p: 0.5108\n",
      "\n",
      "\n",
      "epoch 28 lr 0.001631 loss -1.6046\n",
      "layer norms 1.9061 3.0469 3.1040 0.9906\n",
      "logdet: 2.1020, prior p: 0.4953\n",
      "\n",
      "\n",
      "epoch 29 lr 0.001606 loss -1.6075\n",
      "layer norms 1.9149 3.0966 3.1650 1.0074\n",
      "logdet: 2.1118, prior p: 0.5037\n",
      "sampling complete. Sample mean: -0.7213, std: 0.6459, max: 1.5560, min: -2.1333\n",
      "latent mean: 0.0131, std: 1.0001\n",
      "\n",
      "\n",
      "epoch 30 lr 0.001580 loss -1.6080\n",
      "layer norms 1.9496 3.1906 3.3093 0.9933\n",
      "logdet: 2.1021, prior p: 0.4966\n",
      "\n",
      "\n",
      "epoch 31 lr 0.001554 loss -1.6088\n",
      "layer norms 1.9532 3.2100 3.3450 1.0016\n",
      "logdet: 2.1096, prior p: 0.5008\n",
      "\n",
      "\n",
      "epoch 32 lr 0.001527 loss -1.6094\n",
      "layer norms 1.9438 3.2153 3.3840 0.9946\n",
      "logdet: 2.1101, prior p: 0.4973\n",
      "\n",
      "\n",
      "epoch 33 lr 0.001500 loss -1.6101\n",
      "layer norms 1.9615 3.2624 3.4082 0.9862\n",
      "logdet: 2.1014, prior p: 0.4931\n",
      "\n",
      "\n",
      "epoch 34 lr 0.001473 loss -1.6105\n",
      "layer norms 1.9801 3.3035 3.4854 0.9943\n",
      "logdet: 2.1087, prior p: 0.4971\n",
      "\n",
      "\n",
      "epoch 35 lr 0.001444 loss -1.6111\n",
      "layer norms 1.9916 3.3582 3.5674 1.0076\n",
      "logdet: 2.1105, prior p: 0.5038\n",
      "\n",
      "\n",
      "epoch 36 lr 0.001416 loss -1.6114\n",
      "layer norms 1.9912 3.3812 3.5775 0.9904\n",
      "logdet: 2.1047, prior p: 0.4952\n",
      "\n",
      "\n",
      "epoch 37 lr 0.001387 loss -1.6120\n",
      "layer norms 1.9903 3.3696 3.5706 0.9944\n",
      "logdet: 2.1068, prior p: 0.4972\n",
      "\n",
      "\n",
      "epoch 38 lr 0.001357 loss -1.6126\n",
      "layer norms 1.9986 3.4255 3.6630 0.9972\n",
      "logdet: 2.1101, prior p: 0.4986\n",
      "\n",
      "\n",
      "epoch 39 lr 0.001327 loss -1.6132\n",
      "layer norms 1.9893 3.4284 3.6568 1.0056\n",
      "logdet: 2.1100, prior p: 0.5028\n",
      "sampling complete. Sample mean: -0.7394, std: 0.6230, max: 1.5395, min: -1.6258\n",
      "latent mean: -0.0060, std: 1.0015\n",
      "\n",
      "\n",
      "epoch 40 lr 0.001297 loss -1.6135\n",
      "layer norms 2.0167 3.5094 3.8139 1.0059\n",
      "logdet: 2.1196, prior p: 0.5029\n",
      "\n",
      "\n",
      "epoch 41 lr 0.001267 loss -1.6138\n",
      "layer norms 2.0237 3.5149 3.8169 0.9869\n",
      "logdet: 2.1095, prior p: 0.4935\n",
      "\n",
      "\n",
      "epoch 42 lr 0.001236 loss -1.6142\n",
      "layer norms 2.0398 3.5503 3.8583 1.0059\n",
      "logdet: 2.1141, prior p: 0.5030\n",
      "\n",
      "\n",
      "epoch 43 lr 0.001205 loss -1.6148\n",
      "layer norms 2.0352 3.5254 3.8080 0.9803\n",
      "logdet: 2.1046, prior p: 0.4901\n",
      "\n",
      "\n",
      "epoch 44 lr 0.001174 loss -1.6153\n",
      "layer norms 2.0220 3.5317 3.8837 1.0105\n",
      "logdet: 2.1191, prior p: 0.5052\n",
      "\n",
      "\n",
      "epoch 45 lr 0.001143 loss -1.6157\n",
      "layer norms 2.0243 3.5608 3.8998 0.9930\n",
      "logdet: 2.1162, prior p: 0.4965\n",
      "\n",
      "\n",
      "epoch 46 lr 0.001111 loss -1.6161\n",
      "layer norms 2.0236 3.5691 3.9230 1.0064\n",
      "logdet: 2.1177, prior p: 0.5032\n",
      "\n",
      "\n",
      "epoch 47 lr 0.001080 loss -1.6164\n",
      "layer norms 2.0483 3.6398 3.9974 1.0002\n",
      "logdet: 2.1145, prior p: 0.5001\n",
      "\n",
      "\n",
      "epoch 48 lr 0.001048 loss -1.6168\n",
      "layer norms 2.0635 3.6714 4.0260 1.0049\n",
      "logdet: 2.1157, prior p: 0.5025\n",
      "\n",
      "\n",
      "epoch 49 lr 0.001016 loss -1.6173\n",
      "layer norms 2.0676 3.6764 4.0403 0.9979\n",
      "logdet: 2.1072, prior p: 0.4989\n",
      "sampling complete. Sample mean: -0.7183, std: 0.6450, max: 1.8533, min: -1.6862\n",
      "latent mean: 0.0054, std: 1.0007\n",
      "\n",
      "\n",
      "epoch 50 lr 0.000985 loss -1.6177\n",
      "layer norms 2.0418 3.6343 4.0076 0.9999\n",
      "logdet: 2.1182, prior p: 0.4999\n",
      "\n",
      "\n",
      "epoch 51 lr 0.000953 loss -1.6181\n",
      "layer norms 2.0795 3.7275 4.1077 1.0056\n",
      "logdet: 2.1166, prior p: 0.5028\n",
      "\n",
      "\n",
      "epoch 52 lr 0.000921 loss -1.6185\n",
      "layer norms 2.0597 3.6879 4.0598 0.9980\n",
      "logdet: 2.1180, prior p: 0.4990\n",
      "\n",
      "\n",
      "epoch 53 lr 0.000890 loss -1.6191\n",
      "layer norms 2.0630 3.7263 4.1442 1.0173\n",
      "logdet: 2.1261, prior p: 0.5087\n",
      "\n",
      "\n",
      "epoch 54 lr 0.000858 loss -1.6193\n",
      "layer norms 2.0677 3.7374 4.1534 1.0033\n",
      "logdet: 2.1242, prior p: 0.5017\n",
      "\n",
      "\n",
      "epoch 55 lr 0.000827 loss -1.6198\n",
      "layer norms 2.0533 3.7013 4.1264 0.9973\n",
      "logdet: 2.1207, prior p: 0.4987\n",
      "\n",
      "\n",
      "epoch 56 lr 0.000796 loss -1.6200\n",
      "layer norms 2.0547 3.7290 4.1638 1.0158\n",
      "logdet: 2.1293, prior p: 0.5079\n",
      "\n",
      "\n",
      "epoch 57 lr 0.000765 loss -1.6205\n",
      "layer norms 2.0528 3.7220 4.1595 1.0106\n",
      "logdet: 2.1224, prior p: 0.5053\n",
      "\n",
      "\n",
      "epoch 58 lr 0.000734 loss -1.6209\n",
      "layer norms 2.0589 3.7403 4.1973 1.0058\n",
      "logdet: 2.1219, prior p: 0.5029\n",
      "\n",
      "\n",
      "epoch 59 lr 0.000704 loss -1.6212\n",
      "layer norms 2.0597 3.7486 4.1838 0.9931\n",
      "logdet: 2.1193, prior p: 0.4965\n",
      "sampling complete. Sample mean: -0.7350, std: 0.6260, max: 1.4556, min: -1.7177\n",
      "latent mean: -0.0091, std: 0.9954\n",
      "\n",
      "\n",
      "epoch 60 lr 0.000674 loss -1.6214\n",
      "layer norms 2.0381 3.6991 4.1442 1.0019\n",
      "logdet: 2.1226, prior p: 0.5009\n",
      "\n",
      "\n",
      "epoch 61 lr 0.000644 loss -1.6220\n",
      "layer norms 2.0581 3.7600 4.2308 1.0070\n",
      "logdet: 2.1264, prior p: 0.5035\n",
      "\n",
      "\n",
      "epoch 62 lr 0.000614 loss -1.6222\n",
      "layer norms 2.0797 3.8073 4.2558 0.9978\n",
      "logdet: 2.1182, prior p: 0.4989\n",
      "\n",
      "\n",
      "epoch 63 lr 0.000585 loss -1.6226\n",
      "layer norms 2.0330 3.7213 4.1875 1.0000\n",
      "logdet: 2.1223, prior p: 0.5000\n",
      "\n",
      "\n",
      "epoch 64 lr 0.000557 loss -1.6228\n",
      "layer norms 2.0586 3.7641 4.2345 0.9985\n",
      "logdet: 2.1227, prior p: 0.4992\n",
      "\n",
      "\n",
      "epoch 65 lr 0.000528 loss -1.6232\n",
      "layer norms 2.0564 3.7784 4.2556 0.9933\n",
      "logdet: 2.1202, prior p: 0.4967\n",
      "\n",
      "\n",
      "epoch 66 lr 0.000501 loss -1.6235\n",
      "layer norms 2.0535 3.7789 4.2531 0.9953\n",
      "logdet: 2.1218, prior p: 0.4976\n",
      "\n",
      "\n",
      "epoch 67 lr 0.000474 loss -1.6241\n",
      "layer norms 2.0496 3.7763 4.2739 1.0073\n",
      "logdet: 2.1276, prior p: 0.5036\n",
      "\n",
      "\n",
      "epoch 68 lr 0.000447 loss -1.6243\n",
      "layer norms 2.0533 3.7709 4.2608 0.9895\n",
      "logdet: 2.1209, prior p: 0.4947\n",
      "\n",
      "\n",
      "epoch 69 lr 0.000421 loss -1.6246\n",
      "layer norms 2.0392 3.7636 4.2979 1.0012\n",
      "logdet: 2.1337, prior p: 0.5006\n",
      "sampling complete. Sample mean: -0.7271, std: 0.6340, max: 1.5391, min: -1.6779\n",
      "latent mean: -0.0072, std: 1.0014\n",
      "\n",
      "\n",
      "epoch 70 lr 0.000395 loss -1.6250\n",
      "layer norms 2.0659 3.8225 4.3197 1.0054\n",
      "logdet: 2.1266, prior p: 0.5027\n",
      "\n",
      "\n",
      "epoch 71 lr 0.000370 loss -1.6251\n",
      "layer norms 2.0432 3.7737 4.2618 0.9904\n",
      "logdet: 2.1224, prior p: 0.4952\n",
      "\n",
      "\n",
      "epoch 72 lr 0.000346 loss -1.6256\n",
      "layer norms 2.0734 3.8341 4.3324 0.9987\n",
      "logdet: 2.1293, prior p: 0.4994\n",
      "\n",
      "\n",
      "epoch 73 lr 0.000322 loss -1.6258\n",
      "layer norms 2.0257 3.7341 4.2610 1.0002\n",
      "logdet: 2.1308, prior p: 0.5001\n",
      "\n",
      "\n",
      "epoch 74 lr 0.000299 loss -1.6260\n",
      "layer norms 2.0344 3.7485 4.2618 1.0037\n",
      "logdet: 2.1243, prior p: 0.5019\n",
      "\n",
      "\n",
      "epoch 75 lr 0.000277 loss -1.6264\n",
      "layer norms 2.0644 3.8253 4.3522 0.9979\n",
      "logdet: 2.1253, prior p: 0.4989\n",
      "\n",
      "\n",
      "epoch 76 lr 0.000256 loss -1.6267\n",
      "layer norms 2.0468 3.7832 4.3017 1.0016\n",
      "logdet: 2.1294, prior p: 0.5008\n",
      "\n",
      "\n",
      "epoch 77 lr 0.000235 loss -1.6268\n",
      "layer norms 2.0494 3.7907 4.3158 1.0058\n",
      "logdet: 2.1311, prior p: 0.5029\n",
      "\n",
      "\n",
      "epoch 78 lr 0.000215 loss -1.6271\n",
      "layer norms 2.0259 3.7368 4.2526 0.9867\n",
      "logdet: 2.1246, prior p: 0.4933\n",
      "\n",
      "\n",
      "epoch 79 lr 0.000196 loss -1.6275\n",
      "layer norms 2.0632 3.8203 4.3335 0.9983\n",
      "logdet: 2.1226, prior p: 0.4991\n",
      "sampling complete. Sample mean: -0.7306, std: 0.6332, max: 1.5746, min: -1.6041\n",
      "latent mean: 0.0019, std: 0.9967\n",
      "\n",
      "\n",
      "epoch 80 lr 0.000177 loss -1.6276\n",
      "layer norms 2.0259 3.7397 4.2668 0.9945\n",
      "logdet: 2.1259, prior p: 0.4972\n",
      "\n",
      "\n",
      "epoch 81 lr 0.000160 loss -1.6281\n",
      "layer norms 2.0254 3.7537 4.3048 1.0071\n",
      "logdet: 2.1305, prior p: 0.5036\n",
      "\n",
      "\n",
      "epoch 82 lr 0.000143 loss -1.6282\n",
      "layer norms 2.0275 3.7563 4.2808 1.0011\n",
      "logdet: 2.1265, prior p: 0.5006\n",
      "\n",
      "\n",
      "epoch 83 lr 0.000127 loss -1.6285\n",
      "layer norms 2.0313 3.7651 4.3094 1.0050\n",
      "logdet: 2.1319, prior p: 0.5025\n",
      "\n",
      "\n",
      "epoch 84 lr 0.000112 loss -1.6286\n",
      "layer norms 2.0234 3.7484 4.2771 1.0017\n",
      "logdet: 2.1274, prior p: 0.5008\n",
      "\n",
      "\n",
      "epoch 85 lr 0.000098 loss -1.6289\n",
      "layer norms 2.0128 3.7231 4.2578 1.0003\n",
      "logdet: 2.1297, prior p: 0.5001\n",
      "\n",
      "\n",
      "epoch 86 lr 0.000085 loss -1.6292\n",
      "layer norms 2.0239 3.7542 4.2959 0.9905\n",
      "logdet: 2.1293, prior p: 0.4952\n",
      "\n",
      "\n",
      "epoch 87 lr 0.000073 loss -1.6291\n",
      "layer norms 2.0234 3.7479 4.2801 0.9983\n",
      "logdet: 2.1323, prior p: 0.4991\n",
      "\n",
      "\n",
      "epoch 88 lr 0.000061 loss -1.6296\n",
      "layer norms 2.0260 3.7545 4.2891 0.9995\n",
      "logdet: 2.1287, prior p: 0.4998\n",
      "\n",
      "\n",
      "epoch 89 lr 0.000051 loss -1.6295\n",
      "layer norms 2.0349 3.7639 4.2968 0.9961\n",
      "logdet: 2.1251, prior p: 0.4981\n",
      "sampling complete. Sample mean: -0.7314, std: 0.6323, max: 1.5076, min: -1.7240\n",
      "latent mean: 0.0036, std: 0.9986\n",
      "\n",
      "\n",
      "epoch 90 lr 0.000041 loss -1.6295\n",
      "layer norms 2.0474 3.7935 4.3334 1.0041\n",
      "logdet: 2.1287, prior p: 0.5021\n",
      "\n",
      "\n",
      "epoch 91 lr 0.000033 loss -1.6299\n",
      "layer norms 2.0193 3.7401 4.2705 1.0076\n",
      "logdet: 2.1306, prior p: 0.5038\n",
      "\n",
      "\n",
      "epoch 92 lr 0.000026 loss -1.6298\n",
      "layer norms 2.0228 3.7373 4.2713 1.0026\n",
      "logdet: 2.1317, prior p: 0.5013\n",
      "\n",
      "\n",
      "epoch 93 lr 0.000019 loss -1.6301\n",
      "layer norms 2.0275 3.7522 4.2829 1.0013\n",
      "logdet: 2.1292, prior p: 0.5007\n",
      "\n",
      "\n",
      "epoch 94 lr 0.000014 loss -1.6300\n",
      "layer norms 2.0286 3.7475 4.2851 1.0029\n",
      "logdet: 2.1293, prior p: 0.5014\n",
      "\n",
      "\n",
      "epoch 95 lr 0.000009 loss -1.6301\n",
      "layer norms 2.0045 3.7101 4.2569 0.9990\n",
      "logdet: 2.1337, prior p: 0.4995\n",
      "\n",
      "\n",
      "epoch 96 lr 0.000006 loss -1.6303\n",
      "layer norms 2.0455 3.7893 4.3204 0.9943\n",
      "logdet: 2.1289, prior p: 0.4972\n",
      "\n",
      "\n",
      "epoch 97 lr 0.000003 loss -1.6302\n",
      "layer norms 1.9895 3.6804 4.2255 1.0054\n",
      "logdet: 2.1286, prior p: 0.5027\n",
      "\n",
      "\n",
      "epoch 98 lr 0.000002 loss -1.6303\n",
      "layer norms 2.0160 3.7273 4.2708 0.9986\n",
      "logdet: 2.1302, prior p: 0.4993\n",
      "\n",
      "\n",
      "epoch 99 lr 0.000001 loss -1.6304\n",
      "layer norms 2.0122 3.7244 4.2721 0.9987\n",
      "logdet: 2.1307, prior p: 0.4993\n",
      "sampling complete. Sample mean: -0.7324, std: 0.6308, max: 1.4143, min: -1.6257\n",
      "latent mean: 0.0034, std: 0.9976\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = 'mnist'\n",
    "num_classes = 10\n",
    "img_size = 28\n",
    "channel_size = 1\n",
    "\n",
    "# we use a small model for fast demonstration, increase the model size for better results\n",
    "patch_size = 4\n",
    "channels = 128\n",
    "blocks = 4\n",
    "layers_per_block = 4\n",
    "# try different noise levels to see its effect\n",
    "noise_std = 0.1\n",
    "\n",
    "batch_size = 256\n",
    "lr = 2e-3\n",
    "# increase epochs for better results\n",
    "epochs = 100\n",
    "sample_freq = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda' \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps' # if on mac\n",
    "else:\n",
    "    device = 'cpu' # if mps not available\n",
    "print(f'using device {device}')\n",
    "\n",
    "fixed_noise = torch.randn(num_classes * 10, (img_size // patch_size)**2, channel_size * patch_size ** 2, device=device)\n",
    "fixed_y = torch.arange(num_classes, device=device).view(-1, 1).repeat(1, 10).flatten()\n",
    "\n",
    "transform = tv.transforms.Compose([\n",
    "    tv.transforms.Resize((img_size, img_size)),\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "data = tv.datasets.MNIST('.', transform=transform, train=True, download=True)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "model = Model(in_channels=channel_size, img_size=img_size, patch_size=patch_size, \n",
    "              channels=channels, num_blocks=blocks, layers_per_block=layers_per_block,\n",
    "              num_classes=num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), betas=(0.9, 0.95), lr=lr, weight_decay=1e-4)\n",
    "lr_schedule = utils.CosineLRSchedule(optimizer, len(data_loader), epochs * len(data_loader), 1e-6, lr)\n",
    "\n",
    "model_name = f'original'\n",
    "sample_dir = notebook_output_path / f'{dataset}_samples_{model_name}'\n",
    "ckpt_file = notebook_output_path / f'{dataset}_model_{model_name}.pth'\n",
    "sample_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = 0\n",
    "    for x, y in data_loader:\n",
    "        x = x.to(device)\n",
    "        eps = noise_std * torch.randn_like(x)\n",
    "        x = x + eps\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        z, outputs, logdets = model(x, y)\n",
    "        loss = model.get_loss(z, logdets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_schedule.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    print(f\"epoch {epoch} lr {optimizer.param_groups[0]['lr']:.6f} loss {losses / len(data_loader):.4f}\")\n",
    "    print('layer norms', ' '.join([f'{z.pow(2).mean():.4f}' for z in outputs]))\n",
    "    print(f'logdet: {logdets.mean():.4f}, prior p: {0.5 * z.pow(2).mean():.4f}')\n",
    "    if (epoch + 1) % sample_freq == 0:\n",
    "        with torch.no_grad():\n",
    "            samples = model.reverse(fixed_noise, fixed_y)\n",
    "        sqa_save(samples, sample_dir / f'samples_{epoch:03d}.png')\n",
    "        latents = model.unpatchify(z[:100])\n",
    "        sqa_save(latents, sample_dir / f'latent_{epoch:03d}.png')\n",
    "        print(f'sampling complete. Sample mean: {samples.mean():.4f}, std: {samples.std():.4f}, max: {samples.max():.4f}, min: {samples.min():.4f}')\n",
    "        print(f'latent mean: {latents.mean():.4f}, std: {latents.std():.4f}')\n",
    "    print('\\n')\n",
    "torch.save(model.state_dict(), ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd3fcf68-b9e5-4840-a26e-5091a16e98c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy %98.35\n"
     ]
    }
   ],
   "source": [
    "# now we can also evaluate the model by turning it into a classifier with Bayes rule, p(y|x) = p(y)p(x|y)/p(x)\n",
    "data = tv.datasets.MNIST('.', transform=transform, train=False, download=False)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "num_correct = 0\n",
    "num_examples = 0\n",
    "for x, y in data_loader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    eps = noise_std * torch.randn_like(x)\n",
    "    x = x.repeat(num_classes, 1, 1, 1)\n",
    "    y_ = torch.arange(num_classes, device=device).view(-1, 1).repeat(1, y.size(0)).flatten()\n",
    "    with torch.no_grad():\n",
    "        z, outputs, logdets = model(x, y_)\n",
    "        losses = 0.5 * z.pow(2).mean(dim=[1, 2]) - logdets # keep the batch dimension\n",
    "        pred = losses.reshape(num_classes, y.size(0)).argmin(dim=0)\n",
    "    num_correct += (pred == y).sum()\n",
    "    num_examples += y.size(0)\n",
    "print(f'Accuracy %{100 * num_correct / num_examples:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911a2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# from transformer_flow import Model\n",
    "# import utils\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" # set GPU\n",
    "# utils.set_random_seed(100)\n",
    "\n",
    "# num_classes = 10\n",
    "# img_size = 28\n",
    "# channel_size = 1\n",
    "\n",
    "# # we use a small model for fast demonstration, increase the model size for better results\n",
    "# patch_size = 4\n",
    "# channels = 128\n",
    "# blocks = 4\n",
    "# layers_per_block = 4\n",
    "# # try different noise levels to see its effect\n",
    "# noise_std = 0.1\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = 'cuda' \n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = 'mps' # if on mac\n",
    "# else:\n",
    "#     device = 'cpu' # if mps not available\n",
    "# print(f'using device {device}')\n",
    "\n",
    "# fixed_noise = torch.randn(num_classes * 10, (img_size // patch_size)**2, channel_size * patch_size ** 2, device=device)\n",
    "# # fixed_noise = torch.randn(num_classes * 10, (img_size // patch_size)**2, channels, device=device)\n",
    "# fixed_y = torch.arange(num_classes, device=device).view(-1, 1).repeat(1, 10).flatten()\n",
    "\n",
    "# # load the model\n",
    "# model = Model(in_channels=channel_size, img_size=img_size, patch_size=patch_size, \n",
    "#               channels=channels, num_blocks=blocks, layers_per_block=layers_per_block,\n",
    "#               num_classes=num_classes).to(device)\n",
    "# model.load_state_dict(torch.load(\"runs/notebook/mnist_model_4_128_4_4_0.10_linears_with_residual.pth\"))\n",
    "# model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     samples = model.reverse(fixed_noise, fixed_y)\n",
    "#     # print the mean and std of the samples\n",
    "#     mean = samples.mean(dim=[0, 2, 3])\n",
    "#     std = samples.std(dim=[0, 2, 3])\n",
    "#     print(f'mean: {mean}, std: {std}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
